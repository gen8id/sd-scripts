[general]
shuffle_caption = false
caption_dropout_rate = 0.0
token_warmup_step = 0
caption_tag_dropout_rate = 0.0
caption_extension = ".txt"
keep_tokens = 1
seed = 47

[model]
pretrained_model_name_or_path = "../models/sd_xl_base_1.0.safetensors"
vae = "../models/sdxl.vae.safetensors"

[training]
resolution = "768,768"
batch_size = 1
learning_rate = 1 # ➡️ Prodigy 사용 시 1
lr_scheduler = "cosine_with_restarts" # ➡️ Prodigy와 함께 사용 권장
lr_warmup_steps = 100
max_train_epochs = 15
optimizer_type = "Prodigy" # ➡️ 수렴 속도 개선을 위해 Prodigy 사용
mixed_precision = "bf16" # ➡️ VRAM이 40시리즈라면 품질 개선을 위해 bf16 시도 (아니면 fp16 유지)
gradient_checkpointing = true # ➡️ 필수 유지 (메모리 절약)
clip_skip = 2
network_module = "networks.lora"
network_dim = 64 # ➡️ 표현력 개선을 위해 32에서 64로 상향
network_alpha = 32 # ➡️ dim의 절반으로 설정
save_precision = "bf16" # ➡️ bf16 시도 (아니면 fp16 유지)
save_state = true
save_every_n_epochs = 1
network_train_unet_only = true
cache_text_encoder_outputs = true
no_half_vae = true # VRAM 부족 시 VAE 문제를 해결하는 데 도움이 됨

[folders]
train_data_dir = "../dataset/training"
output_dir = "../output_models"
logging_dir = "../logs"

[advanced]
bucket_reso_steps = 64
bucket_no_upscale = true
xformers = true
cache_latents = true
cache_latents_to_disk = true # VRAM 부족 시 디스크 캐싱은 여전히 안전한 선택
min_bucket_reso = 512
max_bucket_reso = 1024