#!/usr/bin/env python3
"""
SDXL LoRA ì¼ê´„ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- í•™ìŠµ í´ë” í•˜ìœ„ì˜ ì—¬ëŸ¬ ìºë¦­í„°/ê°œë…ì„ ìë™ìœ¼ë¡œ ê°œë³„ LoRA í•™ìŠµ
- VRAMì— ë”°ë¥¸ ìë™ ì„¤ì • (bf16/fp16)
- ì´ë¯¸ì§€ ìˆ˜ì— ë”°ë¥¸ ìµœì  íŒŒë¼ë¯¸í„° ìë™ ê³„ì‚°
"""

import os
import sys
import toml
import math # ìƒë‹¨ì— import math ì¶”ê°€ í•„ìš”
import subprocess
import argparse
from pathlib import Path


class TrainingConfig:
    """í•™ìŠµ ì„¤ì • ê´€ë¦¬"""

    def __init__(self, config_file, gpu_id=0, force_repeats=None):
        self.config_file = config_file
        self.gpu_id = gpu_id
        self.force_repeats = force_repeats

        # VRAM ê°ì§€
        self.vram_size = self.get_vram_size()

        # VRAMì— ë”°ë¥¸ ì„¤ì •
        if self.vram_size >= 20:
            self.config_file = "config-24g.toml"
            self.precision = "bf16"  # í•­ìƒ bf16
            self.target_steps = 1800
        else:
            self.config_file = "config-16g.toml"
            self.precision = "fp16"
            self.target_steps = 1500

        print(f"ğŸ§  VRAM ê°ì§€ ê²°ê³¼: {self.vram_size}GB / Precision={self.precision}")

        # Config íŒŒì¼ ë¡œë“œ
        self.load_config()


    def get_vram_size(self):
        """NVIDIA GPU VRAM í¬ê¸° ê°ì§€ (GB)"""
        try:
            # ëª…ë ¹ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬ (shell=False)
            cmd = [
                "nvidia-smi",
                "--query-gpu=memory.total",
                "--format=csv,noheader,nounits",
                "-i", str(self.gpu_id)
            ]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=True
            )
            vram_mb = int(result.stdout.strip().split("\n")[0])
            vram_gb = vram_mb // 1024
            return vram_gb

        except Exception as e:
            print(f"âš ï¸ VRAM ê°ì§€ ì‹¤íŒ¨ ({e}) â€” ê¸°ë³¸ê°’(24GB, bf16) ì‚¬ìš©")
            self.precision = "bf16"
            return 24

    def load_config(self):
        """config.toml ë¡œë“œ"""
        if not os.path.exists(self.config_file):
            print(f"âŒ Config íŒŒì¼ ì—†ìŒ: {self.config_file}")
            sys.exit(1)

        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config = toml.load(f)

        self.train_dir = self.config['folders']['train_data_dir']
        self.output_dir = self.config['folders']['output_dir']
        self.batch_size = self.config['training'].get('batch_size', 1)


class LoRATrainer:
    """ë‹¨ì¼ LoRA í•™ìŠµ ì‹¤í–‰"""

    def __init__(self, training_config):
        self.config = training_config
        self.output_dir = Path("output")
        self.output_dir.mkdir(exist_ok=True)
        self.image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}

    def find_training_folders(self):
        """í•™ìŠµ í´ë” ì°¾ê¸° (ìˆœì„œ_ì´ë¦„_í´ë˜ìŠ¤ íŒ¨í„´) - ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ì§€ì›"""
        train_dir = self.config.train_dir

        if not os.path.isdir(train_dir):
            print(f"âŒ í•™ìŠµ ë””ë ‰í† ë¦¬ ì—†ìŒ: {train_dir}")
            return []

        folders = []

        # maincharì™€ background í´ë” íƒìƒ‰
        category_folders = ['mainchar', 'background']

        for category in category_folders:
            category_path = os.path.join(train_dir, category)

            if not os.path.isdir(category_path):
                print(f"âš ï¸  ì¹´í…Œê³ ë¦¬ í´ë” ì—†ìŒ: {category_path}")
                continue

            # ê° ì¹´í…Œê³ ë¦¬ ë‚´ë¶€ì˜ í•™ìŠµ í´ë” íƒìƒ‰
            for item in os.listdir(category_path):
                item_path = os.path.join(category_path, item)
                if not os.path.isdir(item_path):
                    continue

                # íŒ¨í„´: 01_alice_woman, 02_bob, 5_style
                parts = item.split('_')  # ì–¸ë”ìŠ¤ì½”ì–´(_)ë¡œ ëª¨ë‘ ë¶„ë¦¬

                # ìµœì†Œ 2ê°œ ìš”ì†Œ(ìˆœì„œ, ì´ë¦„)ê°€ ìˆì–´ì•¼ í•˜ë©°, ì²« ë²ˆì§¸ê°€ ìˆ«ìì—¬ì•¼ í•¨
                if len(parts) >= 2 and parts[0].isdigit():
                    order = int(parts[0])
                    # ì´ë¦„ ì¶”ì¶œ: ë‘ ë²ˆì§¸ ìš”ì†Œë¶€í„° ëê¹Œì§€ë¥¼ ë‹¤ì‹œ ì¡°ì¸
                    # ì˜ˆ: 'alice_woman'
                    name_parts = parts[1:]

                    # ğŸ“Œ ì„¸ ë²ˆì§¸ ìš”ì†Œ (í´ë˜ìŠ¤) ì²˜ë¦¬
                    class_word = None
                    if len(name_parts) >= 2:
                        # ì´ë¦„ê³¼ í´ë˜ìŠ¤ë¥¼ ë¶„ë¦¬
                        # ì´ë¦„: parts[1] (ì˜ˆ: alice)
                        # í´ë˜ìŠ¤: parts[2] (ì˜ˆ: woman)
                        name_token = name_parts[0]
                        class_word = name_parts[1]
                    else:
                        # ì„¸ ë²ˆì§¸ ìš”ì†Œê°€ ì—†ìœ¼ë©´, ì „ì²´ë¥¼ ì´ë¦„ í† í°ìœ¼ë¡œ ì‚¬ìš© (ì˜ˆ: alice)
                        name_token = name_parts[0]

                    folders.append({
                        'order': order,
                        'name': name_token,  # ìˆœìˆ˜ ID (ì˜ˆ: alice)
                        'class': class_word,  # í´ë˜ìŠ¤ (ì˜ˆ: woman) - ì—†ìœ¼ë©´ None
                        'path': item_path,
                        'folder': item,
                        'category': category
                    })

        if not folders:
            print(f"âŒ í•™ìŠµ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print(f"   ê²½ë¡œ: {train_dir}")
            print(f"   ì°¾ëŠ” ìœ„ì¹˜: {train_dir}/mainchar/, {train_dir}/background/")
            print(f"   íŒ¨í„´: N_ID_CLASS ë˜ëŠ” N_ID")
            return []

        # ìˆœì„œëŒ€ë¡œ ì •ë ¬
        folders.sort(key=lambda x: x['order'])

        print(f"âœ… ë°œê²¬ëœ í•™ìŠµ í´ë”: {len(folders)}ê°œ")
        for f in folders:
            class_display = f['class'] if f['class'] else '(None)'
            print(f"   [{f['category']}] {f['order']:02d}_{f['name']}_({class_display})")

        return folders

    def count_images(self, folder_path):
        """í´ë” ë‚´ ì´ë¯¸ì§€ ê°œìˆ˜ ì„¸ê¸°"""
        count = 0
        for file in os.listdir(folder_path):
            if Path(file).suffix.lower() in self.image_extensions:
                count += 1
        return count

    def calculate_training_params(self, image_count):
        """
        ì´ë¯¸ì§€ ìˆ˜, ë°°ì¹˜ ì‚¬ì´ì¦ˆ, ëª©í‘œ ìŠ¤í…ì„ ê¸°ë°˜ìœ¼ë¡œ
        ìµœì ì˜ repeatsì™€ epochsë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (24GB+ VRAM í™˜ê²½)
        """

        # --- 1. ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸° (configì—ì„œ 8, 3000, 10ì„ ê°€ì ¸ì˜¨ë‹¤ê³  ê°€ì •) ---
        batch_size = self.config.batch_size
        target_steps = self.config.target_steps
        force_repeats = getattr(self.config, 'force_repeats', None)
        target_epochs = getattr(self.config, 'target_epochs', 15)  # 15ë¡œ ê³ ì •í•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ ìˆ˜ ì œì–´

        if image_count <= 0 or batch_size <= 0:
            # ì•ˆì „ì¥ì¹˜
            return {"repeats": 1, "epochs": 1, "steps_per_epoch": 1, "total_steps": 1}

        # --- 2. 'ìë™ ê³„ì‚°'ì˜ ê²½ìš° (ì¶”ì²œ ë¡œì§: Epochsë¥¼ ê³ ì •í•˜ê³  Repeats ì—­ì‚°) ---
        if force_repeats is None:

            epochs = target_epochs  # Epochs ê³ ì • (ì˜ˆ: 10)

            # 1. ëª©í‘œ ìŠ¤í… ë‹¬ì„±ì— í•„ìš”í•œ 'repeats'ë¥¼ ì—­ì‚°í•˜ì—¬ ê³„ì‚°
            # repeats = (target_steps * batch_size) / (image_count * epochs)

            denominator = image_count * epochs
            repeats_float = (target_steps * batch_size) / denominator
            # repeatsëŠ” ì •ìˆ˜ì´ë©° ìµœì†Œ 1 (ë°˜ì˜¬ë¦¼ ì‚¬ìš©)
            repeats = max(1, round(repeats_float))

            # 2. ê²°ì •ëœ repeatsë¡œ steps_per_epoch ë‹¤ì‹œ ê³„ì‚° (ì˜¬ë¦¼ ì‚¬ìš©ìœ¼ë¡œ 0 ìŠ¤í… ë°©ì§€)
            steps_per_epoch = math.ceil((image_count * repeats) / batch_size)

        else:
            # --- 3. 'ê°•ì œ repeats'ê°€ ì„¤ì •ëœ ê²½ìš° (ê¸°ì¡´ ë¡œì§) ---
            repeats = force_repeats
            steps_per_epoch = math.ceil((image_count * repeats) / batch_size)
            epochs = max(1, round(target_steps / steps_per_epoch))
            epochs = max(epochs, 5)  # ìµœì†Œ 5 epoch ë³´ì¥

        # --- 4. ìµœì¢… ê²°ê³¼ ê³„ì‚° ---
        total_steps = steps_per_epoch * epochs

        return {
            "repeats": repeats,
            "epochs": epochs,
            "steps_per_epoch": steps_per_epoch,
            "total_steps": total_steps
        }

    def train_single_lora(self, folder_info):
        """ë‹¨ì¼ LoRA í•™ìŠµ"""
        name = folder_info['name']  # ID í† í° (ì˜ˆ: alice)
        class_word = folder_info['class']  # Class í† í° (ì˜ˆ: woman)
        folder = folder_info['folder']
        folder_path = folder_info['path']
        category = folder_info['category']

        # ì´ë¯¸ì§€ ê°œìˆ˜ ê³„ì‚°
        num_images = self.count_images(folder_path)
        if num_images == 0:
            print(f"âŒ {name}: ì´ë¯¸ì§€ ì—†ìŒ")
            return False

        # í•™ìŠµ íŒŒë¼ë¯¸í„° ìë™ ê³„ì‚°
        params = self.calculate_training_params(num_images)
        repeats = params['repeats']
        epochs = params['epochs']

        print(f"\n{'=' * 70}")
        print(f"ğŸ¯ Training LoRA: {name}")
        print(f"{'=' * 70}")
        print(f"ğŸ“Š Training Configuration")
        print(f"{'-' * 70}")
        print(f"  Category:        {category}")
        print(f"  Folder:          {folder}")
        if class_word:
            print(f"  Class Token:     {class_word} (ì •ê·œí™” ì‚¬ìš©)")
        else:
            print(f"  Class Token:     (ì—†ìŒ) (ì •ê·œí™” ë¯¸ì‚¬ìš©)")
        print(f"  Images:          {num_images}")
        print(f"  Repeats:         {repeats} (auto)")
        print(f"  Epochs:          {epochs}")
        print(f"  Total steps:     {params['total_steps']}")  # ìˆ˜ì •: params['total_steps'] ì‚¬ìš©
        print(f"{'-' * 70}")

        # train_data_dirëŠ” ì¹´í…Œê³ ë¦¬ í´ë” (01_alic3_womanì˜ ë¶€ëª¨)
        train_data_dir = os.path.join(self.config.train_dir, category)
        output_name = folder_info.get('output_name', folder)

        cmd = [
            'accelerate', 'launch',
            '--num_cpu_threads_per_process', '1',
            '--mixed_precision', self.config.precision,
            'sdxl_train_network.py',
            f'--config_file={self.config.config_file}',
            f'--train_data_dir={train_data_dir}',  # ì¹´í…Œê³ ë¦¬ í´ë”
            f'--output_name={output_name}',  # ID í† í°ë§Œ ì‚¬ìš©
            f'--max_train_epochs={epochs}',
            f'--dataset_repeats={repeats}'
            f'-logging_dir ../logs/tensorboard'
            f'-report_to tensorboard'
        ]

        # Resume ì²˜ë¦¬
        if hasattr(self.config, 'resume') and self.config.resume:
            # cmd.append(f"--network_weights={self.config.resume}")
            cmd.append(f"--resume={self.config.resume}")
            print(f"   ğŸ”„ Loading weights: {Path(self.config.resume).name}")


        # ğŸ“Œ ì„¸ ë²ˆì§¸ ìš”ì†Œ(Class)ê°€ ìˆì„ ê²½ìš°, --class_tokens ì¸ì ì¶”ê°€
        # if class_word:
        #     # Kohya_SSì˜ Dreambooth/LoRAëŠ” Class í† í°ì„ --class_tokensì— ì „ë‹¬í•˜ì—¬
        #     # ì •ê·œí™” ì´ë¯¸ì§€ í´ë”(ì˜ˆ: reg_woman)ë¥¼ ì°¾ê³  í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        #     cmd.append(f'--class_tokens={class_word}')

        # ì‹¤í–‰
        try:
            env = os.environ.copy()
            env['CUDA_VISIBLE_DEVICES'] = str(self.config.gpu_id)

            print(f"ğŸš€ Starting training...\n")
            print(f"ğŸ“‚ Train dir: {train_data_dir}")
            result = subprocess.run(cmd, env=env, check=True)

            print(f"\nâœ… {name} í•™ìŠµ ì™„ë£Œ!")
            print(f"{'=' * 70}\n")
            return True

        except subprocess.CalledProcessError as e:
            print(f"\nâŒ {name} í•™ìŠµ ì‹¤íŒ¨: {e}")
            print(f"{'=' * 70}\n")
            return False
        except KeyboardInterrupt:
            print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            return False

    def run_batch_training(self):
        """ì¼ê´„ í•™ìŠµ ì‹¤í–‰"""
        folders = self.find_training_folders()

        if not folders:
            print("âŒ í•™ìŠµ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print(f"   ê²½ë¡œ: {self.config.train_dir}")
            print(f"   íŒ¨í„´: 01_name, 02_name, ...")
            return

        print(f"\n{'=' * 70}")
        print(f"ğŸš€ SDXL LoRA Batch Training")
        print(f"{'=' * 70}")
        print(f"ğŸ“ í•™ìŠµ í´ë”: {self.config.train_dir}")
        print(f"ğŸ’¾ ì¶œë ¥ í´ë”: {self.config.output_dir}")
        print(f"ğŸ–¥ï¸ GPU: {self.config.gpu_id} ({self.config.vram_size}GB)")
        print(f"âš¡ Precision: {self.config.precision}")
        print(f"ğŸ“‹ Config: {self.config.config_file}")
        print(f"\në°œê²¬ëœ í•™ìŠµ í´ë”: {len(folders)}ê°œ")
        print(f"{'-' * 70}")
        for f in folders:
            img_count = self.count_images(f['path'])
            print(f"  {f['folder']} ({img_count} images)")
        print(f"{'=' * 70}\n")

        # ì‚¬ìš©ì í™•ì¸
        try:
            response = input("í•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() not in ['y', 'yes']:
                print("âŒ í•™ìŠµ ì·¨ì†Œë¨")
                return
        except KeyboardInterrupt:
            print("\nâŒ í•™ìŠµ ì·¨ì†Œë¨")
            return

        # í•™ìŠµ ì‹¤í–‰
        results = []
        for i, folder in enumerate(folders, 1):
            print(f"\n[{i}/{len(folders)}] Processing: {folder['name']}...")
            success = self.train_single_lora(folder)
            results.append({
                'name': folder['name'],
                'success': success
            })

            # ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰í• ì§€ ë¬¼ì–´ë´„
            if not success:
                try:
                    response = input("â“ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ")
                    if response.lower() in ['n', 'no']:
                        print("âš ï¸ ë‚˜ë¨¸ì§€ í•™ìŠµ ê±´ë„ˆëœ€")
                        break
                except KeyboardInterrupt:
                    print("\nâš ï¸ ë‚˜ë¨¸ì§€ í•™ìŠµ ê±´ë„ˆëœ€")
                    break

        # ê²°ê³¼ ìš”ì•½
        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š Training Summary")
        print(f"{'=' * 70}")
        success_count = sum(1 for r in results if r['success'])
        fail_count = len(results) - success_count

        for r in results:
            status = "âœ…" if r['success'] else "âŒ"
            print(f"{status} {r['name']}")

        print(f"{'-' * 70}")
        print(f"âœ… ì„±ê³µ: {success_count}/{len(results)}")
        if fail_count > 0:
            print(f"âŒ ì‹¤íŒ¨: {fail_count}/{len(results)}")
        print(f"{'=' * 70}\n")


def main():
    parser = argparse.ArgumentParser(
        description="SDXL LoRA ì¼ê´„ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python train_batch.py
  python train_batch.py config-16g.toml
  python train_batch.py config-24g.toml 0 15

í´ë” êµ¬ì¡°:
  training/
  â”œâ”€â”€ 01_alice/
  â”‚   â””â”€â”€ *.jpg
  â”œâ”€â”€ 02_bob/
  â”‚   â””â”€â”€ *.jpg
  â””â”€â”€ 03_background/
      â””â”€â”€ *.jpg
        """
    )

    parser.add_argument(
        "config",
        nargs="?",
        default="config-24g.toml",
        help="Config íŒŒì¼ (ê¸°ë³¸: config-24g.toml)"
    )

    parser.add_argument(
        "gpu_id",
        nargs="?",
        type=int,
        default=0,
        help="GPU ID (ê¸°ë³¸: 0)"
    )

    parser.add_argument(
        "repeats",
        nargs="?",
        type=int,
        default=None,
        help="ê°•ì œ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: ìë™ ê³„ì‚°)"
    )

    args = parser.parse_args()

    try:
        # ì„¤ì • ë¡œë“œ
        training_config = TrainingConfig(
            config_file=args.config,
            gpu_id=args.gpu_id,
            force_repeats=args.repeats
        )

        # í•™ìŠµ ì‹¤í–‰
        trainer = LoRATrainer(training_config)
        trainer.run_batch_training()

    except KeyboardInterrupt:
        print("\n\nâš ï¸ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()