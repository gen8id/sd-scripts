"""
BLIP + WD14 í•˜ì´ë¸Œë¦¬ë“œ ìº¡ì…˜ ìƒì„±ê¸° (ìˆ˜ì • ë²„ì „)
ì‹¤ì‚¬ LoRA í•™ìŠµì„ ìœ„í•œ í†µí•© ìº¡ì…˜ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

í•„ìš” í™˜ê²½: kohya_ss (sd-scripts)
"""
import os
import sys
# í˜„ì¬ íŒŒì¼ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import traceback
from pathlib import Path
import nltk
import numpy as np
import torch
from PIL import Image
from nltk.stem import WordNetLemmatizer
import onnx
import re
import onnxruntime as ort
import logging
from transformers import BlipProcessor, BlipForConditionalGeneration
from library.utils import resize_image

logger = logging.getLogger(__name__)

# ==============================
# âš™ï¸ ì„¤ì • (ìˆ˜ì • ê°€ëŠ¥)
# ==============================
class Config:

    # ë°ì´í„°ì…‹ ê²½ë¡œ
    DATASET_DIRS = [
        "../dataset/training/mainchar",
        "../dataset/training/background",
    ]
    WATCH_DIRS = [
        "../dataset/captioning/mainchar",
        "../dataset/captioning/background",
    ]
    
    # ëª¨ë¸ ì„¤ì •
    BLIP_MODEL_PATH = "Salesforce/blip-image-captioning-large"
    BLIP_CACHE_DIR = "../models/blip-image-captioning-large"
    WD14_MODEL_PATH = "SmilingWolf/wd-v1-4-moat-tagger-v2"
    WD14_CACHE_DIR = "../models/wd-v1-4-moat-tagger-v2"

    # í•™ìŠµ ìµœì í™”ë¥¼ ìœ„í•œ ë¦¬ì‚¬ì´ì¦ˆ ì´ë¯¸ì§€ í¬ê¸°
    IMAGE_SIZE = 448
    
    # WD14 ì„ê³„ê°’ (ìºë¦­í„°)
    WD14_CHARS_GENERAL_THRESHOLD = 0.35
    WD14_CHARS_CHARACTER_THRESHOLD = 0.85

    # WD14 ì„ê³„ê°’ (í’ê²½)
    WD14_BGS_GENERAL_THRESHOLD = 0.20
    WD14_BGS_CHARACTER_THRESHOLD = 0.95

    # BLIP ì„¤ì •
    BLIP_MAX_LENGTH = 75
    BLIP_NUM_BEAMS = 1
    
    # ì œê±°í•  WD14 ë©”íƒ€ íƒœê·¸
    REMOVE_TAGS = [
        "1girl", "1boy", "solo", "looking at viewer",
        "simple background", "white background", "grey background",
        "highres", "absurdres", "lowres", "bad anatomy",
        "signature", "watermark", "artist name", "dated",
        "yuki miku", "snivy", "winter uniform", ":d",
        "rating:safe", "rating:questionable", "rating:explicit",
    ]
    
    # ì¶œë ¥ ì„¤ì •
    OUTPUT_ENCODING = "utf-8"
    OVERWRITE_EXISTING = False
    CREATE_BACKUP = True
    
    # ë””ë°”ì´ìŠ¤
    DEVICE = "cuda"
    
    # ìºë¦­í„° í”„ë¦¬í”½ìŠ¤ (CLIì—ì„œ ì„¤ì •)
    CHARACTER_PREFIX = ""


# ==============================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==============================

class ImageLoadingPrepDataset(torch.utils.data.Dataset):

    def __init__(self, image_paths):
        self.images = image_paths

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = str(self.images[idx])

        try:
            image = Image.open(img_path).convert("RGB")
            image = preprocess_image(image)
            # tensor = torch.tensor(image)
        except Exception as e:
            logger.error(f"Could not load image path: {img_path}, error: {e}")
            return None

        return (image, img_path)

def preprocess_image(image):

    config = Config()
    image = np.array(image)
    image = image[:, :, ::-1]  # RGB->BGR
    # pad to square
    size = max(image.shape[0:2])
    pad_x = size - image.shape[1]
    pad_y = size - image.shape[0]
    pad_l = pad_x // 2
    pad_t = pad_y // 2
    image = np.pad(image, ((pad_t, pad_y - pad_t), (pad_l, pad_x - pad_l), (0, 0)), mode="constant", constant_values=255)
    image = resize_image(image, image.shape[0], image.shape[1], config.IMAGE_SIZE, config.IMAGE_SIZE)
    image = image.astype(np.float32)

    return image


def lemmatize_tags(tags_list):
    """íƒœê·¸ë¥¼ ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜"""
    if lemmatizer is None:
        return tags_list
    return [lemmatizer.lemmatize(tag.lower()) for tag in tags_list]


def normalize_tags(tags_str):
    """íƒœê·¸ ì •ê·œí™”: ê³ ìœ ëª…ì‚¬ ì œê±°, ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì •ë¦¬, ì¤‘ë³µ ì œê±°"""
    if not tags_str:
        return []

    # ìºë¦­í„°ëª… íŒ¨í„´ (ê´„í˜¸ í¬í•¨/ë¯¸í¬í•¨)
    character_patterns = [
        r'^[a-z]+ [a-z]+\s*\([^)]+\)$',  # "ganyu yama (genshin impact)"
        r'^[a-z]+\s*\([^)]+\)$',         # "hutao (genshin impact)" â† ì¶”ê°€ë¨
        r'^[a-z]+ [a-z]+ [a-z]+$'        # "artoria pendragon fate" (3ë‹¨ì–´)
    ]

    # ë¨¼ì € stripë§Œ í•˜ê³  ì›ë³¸ ì¼€ì´ìŠ¤ ìœ ì§€
    tags = [tag.strip().lower() for tag in tags_str.split(',')]
    seen = set()
    unique_tags = []

    for tag in tags:
        if not tag:
            continue
        # ìºë¦­í„°ëª… íŒ¨í„´ ì œê±°
        is_character = any(re.match(pattern, tag) for pattern in character_patterns)
        if is_character:
            print(f"Removed character name: {tag}")
            continue
        if tag not in seen:
            seen.add(tag)
            unique_tags.append(tag)

    return unique_tags


def remove_unwanted_tags(tags_list, remove_list):
    """ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ë””ë²„ê·¸ ë²„ì „)"""
    remove_set = set()
    for tag in remove_list:
        normalized = tag.lower().strip()
        remove_set.add(normalized)
        remove_set.add(normalized.replace(' ', '_'))
        remove_set.add(normalized.replace('_', ' '))

    filtered = []
    removed = []

    for tag in tags_list:
        tag_normalized = tag.lower().strip()

        # ë§¤ì¹­ í™•ì¸
        is_removed = (
                tag_normalized in remove_set or
                tag_normalized.replace(' ', '_') in remove_set or
                tag_normalized.replace('_', ' ') in remove_set
        )

        if is_removed:
            removed.append(tag)
        else:
            filtered.append(tag)

    if removed:
        print(f"[ğŸ—‘ï¸] Removed tags: {', '.join(removed[:5])}{'...' if len(removed) > 5 else ''}")

    return filtered


def merge_captions(blip_caption, wd14_tags):
    """
    BLIP ìº¡ì…˜ê³¼ WD14 íƒœê·¸ ë³‘í•©
    í˜•ì‹: [BLIP ë¬¸ì¥], [WD14 íƒœê·¸ë“¤]
    """
    config = Config()
    # BLIP ì •ê·œí™”
    blip_normalized = blip_caption.strip().lower() if blip_caption else ""
    
    # WD14 íƒœê·¸ ì •ê·œí™” ë° í•„í„°ë§
    wd14_normalized = normalize_tags(wd14_tags)
    wd14_lemmatized = lemmatize_tags(wd14_normalized)
    wd14_filtered = remove_unwanted_tags(wd14_lemmatized, config.REMOVE_TAGS)
    
    # BLIP ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°ìš©)
    blip_words = set(blip_normalized.replace(',', ' ').split()) if blip_normalized else set()
    
    # WD14ì—ì„œ BLIPì— ì´ë¯¸ í¬í•¨ëœ ë‹¨ì–´ ì œê±°
    wd14_deduped = []
    for tag in wd14_filtered:
        # íƒœê·¸ê°€ BLIP ë¬¸ì¥ì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
        if not any(word in tag or tag in word for word in blip_words):
            wd14_deduped.append(tag)
    
    # ìµœì¢… ë³‘í•©: BLIP (ë¬¸ì¥) + WD14 (íƒœê·¸)
    if blip_normalized and wd14_deduped:
        merged = f"{blip_normalized}, {', '.join(wd14_deduped)}"
    elif blip_normalized:
        merged = blip_normalized
    elif wd14_deduped:
        merged = ', '.join(wd14_deduped)
    else:
        merged = ""
    
    return merged


# ==============================
# ğŸ¨ ìº¡ì…˜ ìƒì„± í•¨ìˆ˜
# ==============================

def generate_blip_caption(image_path):
    """BLIPìœ¼ë¡œ ìì—°ì–´ ìº¡ì…˜ ìƒì„±"""
    config = Config()
    try:
        image = Image.open(image_path).convert("RGB")
        inputs = blip_processor(image, return_tensors="pt").to(config.DEVICE)

        with torch.no_grad():
            outputs = blip_model.generate(
                **inputs,
                max_length=config.BLIP_MAX_LENGTH,
                num_beams=config.BLIP_NUM_BEAMS,
            )

        caption = blip_processor.decode(outputs[0], skip_special_tokens=True)

        # BLIP íŠ¹ìˆ˜ ì˜¤ë¥˜ ë‹¨ì–´ ì œê±°
        caption = clean_blip_caption(caption)

        return caption.strip()

    except Exception as e:
        print(f"âš ï¸ BLIP ìƒì„± ì‹¤íŒ¨ ({image_path.name}): {e}")
        return ""


def clean_blip_caption(caption):
    """BLIP ìº¡ì…˜ì—ì„œ ì•Œë ¤ì§„ ì˜¤ë¥˜ ë‹¨ì–´ ì œê±°"""
    if not caption:
        return ""

    # BLIP íŠ¹ìˆ˜ ì˜¤ë¥˜ ë‹¨ì–´ë“¤
    blip_artifacts = [
        "araffe", "arafed", "araffes",  # giraffe ì˜¤ë¥˜
        "blury",  # blurry ì˜¤íƒ€
        "there is a", "there are",  # ë¶ˆí•„ìš”í•œ ì¡´ì¬ í‘œí˜„
        "image of", "picture of",  # ë©”íƒ€ ì„¤ëª…
        "photo of",  # ë©”íƒ€ ì„¤ëª…
    ]

    import re
    cleaned = caption

    for artifact in blip_artifacts:
        # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•´ì„œ ì œê±° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        pattern = r'\b' + re.escape(artifact) + r'\b'
        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)

    # ì—°ì† ê³µë°± ì •ë¦¬
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = cleaned.strip()

    return cleaned


def generate_wd14_tags(image_path):
    """WD14ë¡œ íƒœê·¸ ìƒì„±"""
    config = Config()
    try:
        # WD14Tagger.tag() ë©”ì„œë“œ í˜¸ì¶œ

        if "mainchar" in str(image_path):
            print(f"âš ï¸ CHARACTER")
            general_threshold = config.WD14_CHARS_GENERAL_THRESHOLD
            character_threshold = config.WD14_CHARS_CHARACTER_THRESHOLD
        else:
            print(f"âš ï¸ BACKGROUND")
            general_threshold = config.WD14_BGS_GENERAL_THRESHOLD
            character_threshold = config.WD14_BGS_GENERAL_THRESHOLD

        tags_str = wd14_tagger.tag(
            str(image_path),
            general_threshold=general_threshold,
            character_threshold=character_threshold,
        )
        return tags_str if tags_str else ""
        
    except Exception as e:
        print(f"âš ï¸ WD14 ìƒì„± ì‹¤íŒ¨ ({image_path.name}): {e}")
        return ""


def extract_tag_from_folder(image_path):
    """
    ì´ë¯¸ì§€ ê²½ë¡œì—ì„œ í´ë”ëª… ê¸°ë°˜ íƒœê·¸ ì¶”ì¶œ
    ì˜ˆ: captioning/02_alice/img.jpg â†’ "alice"
    """
    from pathlib import Path

    folder_name = Path(image_path).parent.name

    # íŒ¨í„´: ìˆ«ì_íƒœê·¸ëª… (ì˜ˆ: "02_alice")
    parts = folder_name.split('_', 1)
    if len(parts) == 2 and parts[0].isdigit():
        tag_name = parts[1].strip()
        return tag_name

    return None

# ==============================
# ğŸ“ íŒŒì¼ ì²˜ë¦¬
# ==============================

def get_image_files(directory):
    """ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°"""
    image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
    
    image_files = []
    for ext in image_extensions:
        image_files.extend(Path(directory).glob(f"*{ext}"))
        image_files.extend(Path(directory).glob(f"*{ext.upper()}"))
    
    return sorted(image_files)


def create_backup(caption_path):
    """ê¸°ì¡´ ìº¡ì…˜ íŒŒì¼ ë°±ì—…"""
    if caption_path.exists():
        backup_dir = caption_path.parent / "caption_backup"
        backup_dir.mkdir(exist_ok=True)
        backup_path = backup_dir / caption_path.name
        import shutil
        shutil.copy2(caption_path, backup_path)


# ==============================
# ğŸ·ï¸ WD14 Tagger í´ë˜ìŠ¤
# ==============================

class WD14Tagger:

    def __init__(
        self,
        config,
        model_dir=None,
        repo_id=None,
        onnx=True,
        general_threshold=0.35,
        character_threshold=0.85,
        device=None,
    ):
        self.config = config
        self.model_dir = model_dir or config.WD14_CACHE_DIR
        self.repo_id = repo_id or config.WD14_MODEL_PATH
        self.onnx = onnx
        self.general_threshold = general_threshold
        self.character_threshold = character_threshold
        self.device = device
        # âœ… tag_freq ì´ˆê¸°í™”
        self.tag_freq = {}
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._init_model()

    def _init_model(self):
        """ëª¨ë¸ ë¡œë”© ë° ë ˆì´ë¸” CSV ì²˜ë¦¬"""
        from huggingface_hub import hf_hub_download

        model_location = os.path.join(self.model_dir, self.repo_id.replace("/", "_"))
        os.makedirs(model_location, exist_ok=True)

        # ONNX ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        if self.onnx:
            import onnx
            import onnxruntime as ort

            onnx_path = os.path.join(model_location, "model.onnx")
            if not os.path.exists(onnx_path):
                print(f"    ë‹¤ìš´ë¡œë“œ ì¤‘: {self.repo_id}/model.onnx")
                hf_hub_download(
                    repo_id=self.repo_id, 
                    filename="model.onnx", 
                    local_dir=model_location
                )

            onnx_model = onnx.load(onnx_path)
            self.input_name = onnx_model.graph.input[0].name
            del onnx_model

            # Provider ì„¤ì •
            providers = ["CPUExecutionProvider"]
            available_providers = ort.get_available_providers()
            if "CUDAExecutionProvider" in available_providers:
                providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
            elif "ROCMExecutionProvider" in available_providers:
                providers = ["ROCMExecutionProvider", "CPUExecutionProvider"]

            self.ort_sess = ort.InferenceSession(onnx_path, providers=providers)

        # CSV ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
        csv_file = os.path.join(model_location, "selected_tags.csv")
        if not os.path.exists(csv_file):
            print(f"    ë‹¤ìš´ë¡œë“œ ì¤‘: {self.repo_id}/selected_tags.csv")
            hf_hub_download(
                repo_id=self.repo_id,
                filename="selected_tags.csv",
                local_dir=model_location
            )

        import csv
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            lines = list(reader)
        
        header, rows = lines[0], lines[1:]
        assert header[0] == "tag_id" and header[1] == "name" and header[2] == "category"

        self.rating_tags = [row[1] for row in rows if row[2] == "9"]
        self.general_tags = [row[1] for row in rows if row[2] == "0"]
        self.character_tags = [row[1] for row in rows if row[2] == "4"]

    def tag(self, image_path, general_threshold=None, character_threshold=None):
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ íƒœê¹… - íƒœê·¸ ë¬¸ìì—´ ë°˜í™˜
        """
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            img = Image.open(image_path).convert("RGB")
            img_array = preprocess_image(img)
            img_array = np.expand_dims(img_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            
            # ì¶”ë¡ 
            preds = self.ort_sess.run(None, {self.input_name: img_array})[0]
            
            # Threshold ì„¤ì •
            gen_thresh = general_threshold if general_threshold is not None else self.general_threshold
            char_thresh = character_threshold if character_threshold is not None else self.character_threshold
            
            # íƒœê·¸ ì¶”ì¶œ
            tags = []
            
            # Character íƒœê·¸ ë¨¼ì € (ìˆìœ¼ë©´)
            for i, tag in enumerate(self.character_tags):
                if preds[0][i] >= char_thresh:
                    tags.append(tag.replace('_', ' '))
            
            # General íƒœê·¸
            for i, tag in enumerate(self.general_tags):
                if preds[0][len(self.character_tags) + i] >= gen_thresh:
                    tags.append(tag.replace('_', ' '))
            
            return ', '.join(tags)
            
        except Exception as e:
            print(f"âš ï¸ WD14 íƒœê¹… ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return ""


def load_models(config):

    try:

        print("  â†’ NLTK ëª¨ë¸ ë¡œë”©...")
        # sd-scripts ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
        nltk_models_dir = os.path.join(os.path.dirname(__file__), "..", "models", "nltk_data")
        os.makedirs(nltk_models_dir, exist_ok=True)

        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        nltk.data.path.append(nltk_models_dir)
        nltk.download('wordnet', download_dir=nltk_models_dir, quiet=True)
        nltk.download('omw-1.4', download_dir=nltk_models_dir, quiet=True)
        
        global characters
        global lemmatizer
        global blip_processor
        global blip_model
        global wd14_tagger

        lemmatizer = WordNetLemmatizer()
        # BLIP ë¡œë“œ
        print("  â†’ BLIP ëª¨ë¸ ë¡œë”©...")
        blip_processor = BlipProcessor.from_pretrained(
            config.BLIP_MODEL_PATH,
            cache_dir=config.BLIP_CACHE_DIR
        )
        
        blip_model = BlipForConditionalGeneration.from_pretrained(
            config.BLIP_MODEL_PATH,
            cache_dir=config.BLIP_CACHE_DIR
        ).to(config.DEVICE)
        blip_model.eval()

        # WD14 ë¡œë“œ
        print("  â†’ WD14 Tagger ë¡œë”©...")
        wd14_tagger = WD14Tagger(
            config=config,
            model_dir=config.WD14_CACHE_DIR,
            general_threshold=config.WD14_CHARS_GENERAL_THRESHOLD,
            character_threshold=config.WD14_CHARS_CHARACTER_THRESHOLD,
        )

        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        sys.exit(1)


def generate_caption(image_path):
    """ì´ë¯¸ì§€ì— ëŒ€í•œ ìº¡ì…˜ ìƒì„± (í´ë” íƒœê·¸ ìë™ ì¶”ê°€)"""
    config = Config()
    caption_path = image_path.with_suffix('.txt')

    # ê¸°ì¡´ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if caption_path.exists() and not config.OVERWRITE_EXISTING:
        return 0

    # ë°±ì—… ìƒì„±
    if config.CREATE_BACKUP and caption_path.exists():
        create_backup(caption_path)

    # 1. BLIP ìº¡ì…˜ ìƒì„±
    blip_caption = generate_blip_caption(image_path)

    # 2. WD14 íƒœê·¸ ìƒì„±
    wd14_tags = generate_wd14_tags(image_path)

    # 3. ë³‘í•©
    merged_caption = merge_captions(blip_caption, wd14_tags)

    # âœ¨ 4. í´ë”ëª…ì—ì„œ íƒœê·¸ ì¶”ì¶œ ë° ì¶”ê°€ (NEW!)
    folder_tag = extract_tag_from_folder(image_path)
    if folder_tag:
        merged_caption = f"{folder_tag}, {merged_caption}"
        print(f"  [ğŸ“Œ] Added folder tag: '{folder_tag}'")
    # 5. ëŒ€ì²´: CHARACTER_PREFIX ì‚¬ìš© (í´ë” íƒœê·¸ ì—†ì„ ë•Œë§Œ)
    elif config.CHARACTER_PREFIX:
        char_token = config.CHARACTER_PREFIX.strip()
        merged_caption = f"{char_token}, {merged_caption}"
        print(f"  [ğŸ·ï¸] Added prefix: '{char_token}'")

    # 6. ì €ì¥
    if merged_caption:
        with open(caption_path, 'w', encoding=config.OUTPUT_ENCODING) as f:
            f.write(merged_caption)
        print(f"[âœ…] Caption saved")
        return 1
    else:
        print(f"âš ï¸ ë¹ˆ ìº¡ì…˜: {image_path.name}")
        return 0


# ==============================
# ğŸ·ï¸ WD14 Tagger í´ë˜ìŠ¤
# ==============================
class WD14Tagger:

    def __init__(
            self,
            config,
            model_dir=None,
            repo_id=None,
            onnx=True,
            general_threshold=0.35,
            character_threshold=0.85,
            device=None,
    ):
        self.config = config
        self.model_dir = model_dir or config.WD14_CACHE_DIR
        self.repo_id = repo_id or config.WD14_MODEL_PATH
        self.onnx = onnx
        self.general_threshold = general_threshold
        self.character_threshold = character_threshold
        self.device = device

        # âœ… tag_freq ì´ˆê¸°í™”
        self.tag_freq = {}

        # ëª¨ë¸ ì´ˆê¸°í™”
        self._init_model()

    def _init_model(self):
        """ëª¨ë¸ ë¡œë”© ë° ë ˆì´ë¸” CSV ì²˜ë¦¬"""
        from huggingface_hub import hf_hub_download

        model_location = os.path.join(self.model_dir, self.repo_id.replace("/", "_"))
        os.makedirs(model_location, exist_ok=True)

        # ONNX ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        if self.onnx:
            onnx_path = os.path.join(model_location, "model.onnx")
            if not os.path.exists(onnx_path):
                print(f"    ë‹¤ìš´ë¡œë“œ ì¤‘: {self.repo_id}/model.onnx")
                hf_hub_download(
                    repo_id=self.repo_id,
                    filename="model.onnx",
                    local_dir=model_location
                )

            model = onnx.load(onnx_path)
            self.input_name = model.graph.input[0].name
            del model

            # Provider ì„¤ì •
            providers = ["CPUExecutionProvider"]
            available_providers = ort.get_available_providers()
            if "CUDAExecutionProvider" in available_providers:
                providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
            elif "ROCMExecutionProvider" in available_providers:
                providers = ["ROCMExecutionProvider", "CPUExecutionProvider"]

            self.ort_sess = ort.InferenceSession(onnx_path, providers=providers)

        # CSV ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
        csv_file = os.path.join(model_location, "selected_tags.csv")
        if not os.path.exists(csv_file):
            print(f"    ë‹¤ìš´ë¡œë“œ ì¤‘: {self.repo_id}/selected_tags.csv")
            hf_hub_download(
                repo_id=self.repo_id,
                filename="selected_tags.csv",
                local_dir=model_location
            )

        import csv
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            lines = list(reader)

        header, rows = lines[0], lines[1:]
        assert header[0] == "tag_id" and header[1] == "name" and header[2] == "category"

        self.rating_tags = [row[1] for row in rows if row[2] == "9"]
        self.general_tags = [row[1] for row in rows if row[2] == "0"]
        self.character_tags = [row[1] for row in rows if row[2] == "4"]

    def tag(self, image_path, general_threshold=None, character_threshold=None):
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ íƒœê¹… - íƒœê·¸ ë¬¸ìì—´ ë°˜í™˜
        """
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            img = Image.open(image_path).convert("RGB")
            img_array = preprocess_image(img)
            img_array = np.expand_dims(img_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

            # ì¶”ë¡ 
            preds = self.ort_sess.run(None, {self.input_name: img_array})[0]

            # Threshold ì„¤ì •
            gen_thresh = general_threshold if general_threshold is not None else self.general_threshold
            char_thresh = character_threshold if character_threshold is not None else self.character_threshold

            # íƒœê·¸ ì¶”ì¶œ
            tags = []

            # Character íƒœê·¸ ë¨¼ì € (ìˆìœ¼ë©´)
            for i, tag in enumerate(self.character_tags):
                if preds[0][i] >= char_thresh:
                    tags.append(tag.replace('_', ' '))

            # General íƒœê·¸
            for i, tag in enumerate(self.general_tags):
                if preds[0][len(self.character_tags) + i] >= gen_thresh:
                    tags.append(tag.replace('_', ' '))

            return ', '.join(tags)

        except Exception as e:
            print(f"âš ï¸ WD14 íƒœê¹… ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return ""
