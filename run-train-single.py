#!/usr/bin/env python3
"""
SDXL LoRA Îã®Ïùº ÌïôÏäµ Ïä§ÌÅ¨Î¶ΩÌä∏ (Í≥†Í∏â ÏÇ¨Ïö©ÏûêÏö©)
- ÌäπÏ†ï Ìè¥ÎçîÎßå ÏÑ†ÌÉù ÌïôÏäµ
- ÏÑ∏Î∞ÄÌïú ÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ï†ï Í∞ÄÎä•
- Config Ïò§Î≤ÑÎùºÏù¥Îìú
"""

import os
import sys
import json
import toml
import subprocess
import argparse
from pathlib import Path


def get_vram_size(gpu_id=0):
    """NVIDIA GPU VRAM ÌÅ¨Í∏∞ Í∞êÏßÄ (GB)"""
    try:
        cmd = [
            "nvidia-smi",
            "--query-gpu=memory.total",
            "--format=csv,noheader,nounits",
            "-i", str(gpu_id)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        vram_mb = int(result.stdout.strip())
        return vram_mb // 1024
    except:
        return 24  # Í∏∞Î≥∏Í∞í


def count_images(folder_path):
    """Ìè¥Îçî ÎÇ¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò ÏÑ∏Í∏∞"""
    extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
    count = 0
    for file in folder_path.iterdir():
        if file.suffix.lower() in extensions:
            count += 1
    return count


def calculate_auto_params(image_count, vram_size, batch_size=1):
    """Ïù¥ÎØ∏ÏßÄ Ïàò Í∏∞Î∞ò ÏûêÎèô ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞"""
    target_steps = 1800 if vram_size >= 20 else 1500

    # Repeats Í≥ÑÏÇ∞
    if image_count < 20:
        repeats = max(80, min(200, target_steps // (image_count * 10)))
    elif image_count < 50:
        repeats = max(30, min(80, target_steps // (image_count * 10)))
    elif image_count < 100:
        repeats = max(15, min(30, target_steps // (image_count * 10)))
    else:
        repeats = max(5, min(20, target_steps // (image_count * 10)))

    # Epochs Í≥ÑÏÇ∞
    images_per_epoch = image_count * repeats
    steps_per_epoch = images_per_epoch // batch_size
    epochs = max(1, round(target_steps / steps_per_epoch))
    epochs = min(max(epochs, 5), 30)

    return {
        'repeats': repeats,
        'epochs': epochs,
        'steps_per_epoch': steps_per_epoch,
        'total_steps': epochs * steps_per_epoch
    }


def main():
    parser = argparse.ArgumentParser(
        description="SDXL LoRA Îã®Ïùº ÌïôÏäµ (Í≥†Í∏â ÏÑ§Ï†ï)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ÏÇ¨Ïö© ÏòàÏãú:
  # Í∏∞Î≥∏ (ÏûêÎèô Í≥ÑÏÇ∞)
  python train_single.py --folder ../dataset/training/01_alice

  # ÏàòÎèô ÌååÎùºÎØ∏ÌÑ∞ ÏßÄÏ†ï
  python train_single.py --folder ../dataset/training/01_alice --epochs 20 --repeats 30

  # Learning rate Ï°∞Ï†ï
  python train_single.py --folder ../dataset/training/01_alice --lr 0.0002

  # Network dim Î≥ÄÍ≤Ω
  python train_single.py --folder ../dataset/training/01_alice --dim 64 --alpha 32

  # Ïù¥Ïñ¥ÏÑú ÌïôÏäµ (Resume)
  python train_single.py --folder ../dataset/training/01_alice --resume ../output_models/alice-epoch-010.safetensors

  # Resume + ÌååÎùºÎØ∏ÌÑ∞ Î≥ÄÍ≤Ω
  python train_single.py --folder ../dataset/training/01_alice --resume ../output_models/alice-epoch-010.safetensors --epochs 30 --lr 0.00008

  # Ï†ÑÏ≤¥ Ïª§Ïä§ÌÖÄ
  python train_single.py \\
    --folder ../dataset/training/01_alice \\
    --output alice_v2 \\
    --config config-24g.toml \\
    --gpu 0 \\
    --epochs 25 \\
    --repeats 40 \\
    --lr 0.00015 \\
    --dim 64 \\
    --alpha 32 \\
    --batch-size 2
        """
    )

    # ÌïÑÏàò Ïù∏Ïûê
    parser.add_argument(
        "--folder",
        required=True,
        help="ÌïôÏäµÌï† Ìè¥Îçî Í≤ΩÎ°ú (Ïòà: ../dataset/training/01_alice)"
    )

    # Í∏∞Î≥∏ ÏÑ§Ï†ï
    parser.add_argument(
        "--config",
        default="config-24g.toml",
        help="Config ÌååÏùº (Í∏∞Î≥∏: config-24g.toml)"
    )

    parser.add_argument(
        "--output",
        help="Ï∂úÎ†• LoRA Ïù¥Î¶Ñ (Í∏∞Î≥∏: Ìè¥ÎçîÎ™ÖÏóêÏÑú Ï∂îÏ∂ú)"
    )

    parser.add_argument(
        "--gpu",
        type=int,
        default=0,
        help="GPU ID (Í∏∞Î≥∏: 0)"
    )

    # ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞
    parser.add_argument(
        "--epochs",
        type=int,
        help="Ï¥ù Epoch Ïàò (Í∏∞Î≥∏: ÏûêÎèô Í≥ÑÏÇ∞)"
    )

    parser.add_argument(
        "--repeats",
        type=int,
        help="Ïù¥ÎØ∏ÏßÄ Î∞òÎ≥µ ÌöüÏàò (Í∏∞Î≥∏: ÏûêÎèô Í≥ÑÏÇ∞)"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        help="Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à (Í∏∞Î≥∏: config Í∞í)"
    )

    parser.add_argument(
        "--lr",
        type=float,
        help="Learning rate (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 1e-4)"
    )

    parser.add_argument(
        "--dim",
        type=int,
        help="Network dimension (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 32)"
    )

    parser.add_argument(
        "--alpha",
        type=int,
        help="Network alpha (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 16)"
    )

    parser.add_argument(
        "--resolution",
        help="Ìï¥ÏÉÅÎèÑ (Ïòà: 1024,1024 ÎòêÎäî 768,768)"
    )

    parser.add_argument(
        "--save-every",
        type=int,
        help="N epochÎßàÎã§ Ï†ÄÏû• (Í∏∞Î≥∏: config Í∞í)"
    )

    # Í≥†Í∏â ÏòµÏÖò
    parser.add_argument(
        "--optimizer",
        help="Optimizer (Ïòà: AdamW8bit, Lion, Prodigy)"
    )

    parser.add_argument(
        "--scheduler",
        help="LR Scheduler (Ïòà: cosine, constant, polynomial)"
    )

    parser.add_argument(
        "--no-auto",
        action="store_true",
        help="ÏûêÎèô Í≥ÑÏÇ∞ ÎπÑÌôúÏÑ±Ìôî (epochs/repeats ÏàòÎèô ÏßÄÏ†ï ÌïÑÏàò)"
    )

    # Resume ÏòµÏÖò
    parser.add_argument(
        "--resume",
        help="Ïù¥Ïñ¥ÏÑú ÌïôÏäµÌï† LoRA ÌååÏùº Í≤ΩÎ°ú (Ïòà: ../output_models/alice-epoch-010.safetensors)"
    )

    args = parser.parse_args()

    # Ìè¥Îçî ÌôïÏù∏
    folder_path = Path(args.folder)
    if not folder_path.exists() or not folder_path.is_dir():
        print(f"‚ùå Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {folder_path}")
        sys.exit(1)

    # Ïù¥ÎØ∏ÏßÄ Í∞úÏàò
    image_count = count_images(folder_path)
    if image_count == 0:
        print(f"‚ùå Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§: {folder_path}")
        sys.exit(1)

    # VRAM Í∞êÏßÄ
    vram_size = get_vram_size(args.gpu)

    # Config ÏûêÎèô ÏÑ†ÌÉù
    if vram_size >= 20:
        precision = "bf16"
        config_file = args.config if args.config else "config-24g.toml"
    else:
        precision = "fp16"
        config_file = args.config if args.config else "config-16g.toml"
        print(f"‚ö†Ô∏è VRAM {vram_size}GB < 20GB, fp16 Î™®ÎìúÎ°ú Ï†ÑÌôò")

    # Config Î°úÎìú
    if not os.path.exists(config_file):
        print(f"‚ùå Config ÌååÏùº ÏóÜÏùå: {config_file}")
        sys.exit(1)

    with open(config_file, 'r', encoding='utf-8') as f:
        config = toml.load(f)

    batch_size = args.batch_size or config['training'].get('batch_size', 1)

    # Ï∂úÎ†• Ïù¥Î¶Ñ Í≤∞Ï†ï
    if args.output:
        output_name = args.output
    else:
        # Ìè¥ÎçîÎ™ÖÏóêÏÑú Ï∂îÏ∂ú (01_alice ‚Üí alice)
        folder_name = folder_path.name
        parts = folder_name.split('_', 1)
        if len(parts) == 2 and parts[0].isdigit():
            output_name = parts[1]
        else:
            output_name = folder_name

    # ÌååÎùºÎØ∏ÌÑ∞ Í≤∞Ï†ï
    if args.no_auto:
        # ÏàòÎèô Î™®Îìú
        if not args.epochs or not args.repeats:
            print("‚ùå --no-auto ÏÇ¨Ïö© Ïãú --epochsÏôÄ --repeats ÌïÑÏàòÏûÖÎãàÎã§")
            sys.exit(1)
        epochs = args.epochs
        repeats = args.repeats
        steps_per_epoch = (image_count * repeats) // batch_size
        total_steps = epochs * steps_per_epoch
    else:
        # ÏûêÎèô Í≥ÑÏÇ∞ (Ïò§Î≤ÑÎùºÏù¥Îìú Í∞ÄÎä•)
        auto_params = calculate_auto_params(image_count, vram_size, batch_size)
        epochs = args.epochs or auto_params['epochs']
        repeats = args.repeats or auto_params['repeats']
        steps_per_epoch = (image_count * repeats) // batch_size
        total_steps = epochs * steps_per_epoch

    # ÌïôÏäµ Ï†ïÎ≥¥ Ï∂úÎ†•
    print(f"\n{'=' * 70}")
    print(f"üéØ SDXL LoRA Training - Single Mode")
    print(f"{'=' * 70}")
    print(f"üìÅ Folder:         {folder_path}")
    print(f"üíæ Output:         {output_name}.safetensors")
    print(f"üìã Config:         {config_file}")
    print(f"üñ•Ô∏è  GPU:            {args.gpu} ({vram_size}GB VRAM)")
    print(f"‚ö° Precision:      {precision}")

    # Resume Ï†ïÎ≥¥
    if args.resume:
        if not os.path.exists(args.resume):
            print(f"‚ùå Resume ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {args.resume}")
            sys.exit(1)
        print(f"üîÑ Resume from:    {args.resume}")

    print(f"{'-' * 70}")
    print(f"üìä Training Parameters")
    print(f"{'-' * 70}")
    print(f"  Images:          {image_count}")
    print(f"  Repeats:         {repeats}" + (" (manual)" if args.repeats else " (auto)"))
    print(f"  Epochs:          {epochs}" + (" (manual)" if args.epochs else " (auto)"))
    print(f"  Batch size:      {batch_size}" + (" (override)" if args.batch_size else ""))
    print(f"  Images/epoch:    {image_count * repeats}")
    print(f"  Steps/epoch:     {steps_per_epoch}")
    print(f"  Total steps:     {total_steps}")

    # Ïò§Î≤ÑÎùºÏù¥ÎìúÎêú ÌååÎùºÎØ∏ÌÑ∞ ÌëúÏãú
    overrides = []
    if args.lr:
        print(f"  Learning rate:   {args.lr} (override)")
        overrides.append(('lr', args.lr))
    if args.dim:
        print(f"  Network dim:     {args.dim} (override)")
        overrides.append(('dim', args.dim))
    if args.alpha:
        print(f"  Network alpha:   {args.alpha} (override)")
        overrides.append(('alpha', args.alpha))
    if args.resolution:
        print(f"  Resolution:      {args.resolution} (override)")
        overrides.append(('resolution', args.resolution))
    if args.optimizer:
        print(f"  Optimizer:       {args.optimizer} (override)")
        overrides.append(('optimizer', args.optimizer))
    if args.scheduler:
        print(f"  LR Scheduler:    {args.scheduler} (override)")
        overrides.append(('scheduler', args.scheduler))
    if args.save_every:
        print(f"  Save every:      {args.save_every} epochs (override)")
        overrides.append(('save_every', args.save_every))

    print(f"{'=' * 70}\n")

    # ÏÇ¨Ïö©Ïûê ÌôïÏù∏
    try:
        response = input("ÌïôÏäµÏùÑ ÏãúÏûëÌïòÏãúÍ≤†ÏäµÎãàÍπå? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("‚ùå ÌïôÏäµ Ï∑®ÏÜåÎê®")
            sys.exit(0)
    except KeyboardInterrupt:
        print("\n‚ùå ÌïôÏäµ Ï∑®ÏÜåÎê®")
        sys.exit(0)

    # accelerate Î™ÖÎ†πÏñ¥ Íµ¨ÏÑ±
    cmd = [
        "accelerate", "launch",
        "--num_cpu_threads_per_process", "1",
        "--mixed_precision", precision,
        "sdxl_train_network.py",
        f"--config_file={config_file}",
        f"--train_data_dir={folder_path}",
        f"--output_name={output_name}",
        f"--max_train_epochs={epochs}",
        f"--dataset_repeats={repeats}"
    ]

    # resume(Ïù¥Ïñ¥Î∞õÍ∏∞) Ï≤òÎ¶¨ ‚Äî ÏïàÏ†ÑÌïòÍ≤å
    # ÎåÄÎ∂ÄÎ∂ÑÏùò Í≤ΩÏö∞ users want to load model weights -> use --network_weights
    # Only use --resume if you have an accelerator checkpoint (full state)
    # resume Í∞íÏù¥ Ïã§Ï†úÎ°ú Ï°¥Ïû¨ÌïòÎäî ÌååÏùºÏùº ÎïåÎßå Ï†ÑÎã¨
    if args.resume:
        if os.path.exists(args.resume):
            # ÎåÄÎ∂ÄÎ∂ÑÏùò Í≤ΩÏö∞ model weights Î°úÎìúÍ∞Ä Î™©Ï†ÅÏù¥ÎùºÎ©¥ network_weightsÎ°ú Ï†ÑÎã¨
            cmd.append(f"--network_weights={str(args.resume)}")
            print(f"\nüîÑ Resuming weights from: {args.resume}\n")
        else:
            print(f"‚ùå Resume ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {args.resume}")
            sys.exit(1)

    # Ïò§Î≤ÑÎùºÏù¥Îìú Ï∂îÍ∞Ä
    if args.batch_size:
        cmd.append(f"--train_batch_size={args.batch_size}")
    if args.lr:
        cmd.append(f"--learning_rate={args.lr}")
    if args.dim:
        cmd.append(f"--network_dim={args.dim}")
    if args.alpha:
        cmd.append(f"--network_alpha={args.alpha}")
    if args.resolution:
        cmd.append(f"--resolution={args.resolution}")
    if args.optimizer:
        cmd.append(f"--optimizer_type={args.optimizer}")
    if args.scheduler:
        cmd.append(f"--lr_scheduler={args.scheduler}")
    if args.save_every:
        cmd.append(f"--save_every_n_epochs={args.save_every}")

    # Resume Ï∂îÍ∞Ä
    if args.resume:
        cmd.append(f"--network_weights={args.resume}")
        print(f"\nüîÑ Resuming from: {args.resume}\n")

    # ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = str(args.gpu)

    # Ïã§Ìñâ
    try:
        print(f"\nüöÄ Starting training...\n")
        subprocess.run(cmd, env=env, check=True)
        print(f"\n‚úÖ ÌïôÏäµ ÏôÑÎ£å: {output_name}.safetensors")
        print(f"{'=' * 70}\n")

    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå ÌïôÏäµ Ïã§Ìå®: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è ÌïôÏäµ Ï§ëÎã®Îê®")
        sys.exit(1)


if __name__ == "__main__":
    main()