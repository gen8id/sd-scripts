#!/usr/bin/env python3
"""
SDXL LoRA ì¼ê´„ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- í•™ìŠµ í´ë” í•˜ìœ„ì˜ ì—¬ëŸ¬ ìºë¦­í„°/ê°œë…ì„ ìë™ìœ¼ë¡œ ê°œë³„ LoRA í•™ìŠµ
- VRAMì— ë”°ë¥¸ ìë™ ì„¤ì • (bf16/fp16)
- ì´ë¯¸ì§€ ìˆ˜ì— ë”°ë¥¸ ìµœì  íŒŒë¼ë¯¸í„° ìë™ ê³„ì‚°
"""

import os
import sys
import toml
import json
import subprocess
import argparse
from pathlib import Path


class TrainingConfig:
    """í•™ìŠµ ì„¤ì • ê´€ë¦¬"""

    def __init__(self, config_file, gpu_id=0, force_repeats=None):
        self.config_file = config_file
        self.gpu_id = gpu_id
        self.force_repeats = force_repeats

        # VRAM ê°ì§€
        self.vram_size = self.get_vram_size()

        # VRAMì— ë”°ë¥¸ ì„¤ì •
        if self.vram_size >= 20:
            self.config_file = "config-24g.toml"
            self.precision = "bf16"  # í•­ìƒ bf16
            self.target_steps = 1800
        else:
            self.config_file = "config-16g.toml"
            self.precision = "fp16"
            self.target_steps = 1500

        print(f"ğŸ§  VRAM ê°ì§€ ê²°ê³¼: {self.vram_size}GB / Precision={self.precision}")

        # Config íŒŒì¼ ë¡œë“œ
        self.load_config()


    def get_vram_size(self):
        """NVIDIA GPU VRAM í¬ê¸° ê°ì§€ (GB)"""
        try:
            # ëª…ë ¹ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬ (shell=False)
            cmd = [
                "nvidia-smi",
                "--query-gpu=memory.total",
                "--format=csv,noheader,nounits",
                "-i", str(self.gpu_id)
            ]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=True
            )
            vram_mb = int(result.stdout.strip().split("\n")[0])
            vram_gb = vram_mb // 1024
            return vram_gb

        except Exception as e:
            print(f"âš ï¸ VRAM ê°ì§€ ì‹¤íŒ¨ ({e}) â€” ê¸°ë³¸ê°’(24GB, bf16) ì‚¬ìš©")
            self.precision = "bf16"
            return 24

    def load_config(self):
        """config.toml ë¡œë“œ"""
        if not os.path.exists(self.config_file):
            print(f"âŒ Config íŒŒì¼ ì—†ìŒ: {self.config_file}")
            sys.exit(1)

        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config = toml.load(f)

        self.train_dir = self.config['folders']['train_data_dir']
        self.output_dir = self.config['folders']['output_dir']
        self.batch_size = self.config['training'].get('batch_size', 1)


class LoRATrainer:
    """ë‹¨ì¼ LoRA í•™ìŠµ ì‹¤í–‰"""

    def __init__(self, training_config):
        self.config = training_config
        self.image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}


    def find_training_folders(self):
        """í•™ìŠµ í´ë” ì°¾ê¸° (ìˆœì„œ_ì´ë¦„ íŒ¨í„´) - ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ì§€ì›"""
        train_dir = self.config.train_dir

        if not os.path.isdir(train_dir):
            print(f"âŒ í•™ìŠµ ë””ë ‰í† ë¦¬ ì—†ìŒ: {train_dir}")
            return []

        folders = []

        # maincharì™€ background í´ë” íƒìƒ‰
        category_folders = ['mainchar', 'background']

        for category in category_folders:
            category_path = os.path.join(train_dir, category)

            if not os.path.isdir(category_path):
                print(f"âš ï¸  ì¹´í…Œê³ ë¦¬ í´ë” ì—†ìŒ: {category_path}")
                continue

            # ê° ì¹´í…Œê³ ë¦¬ ë‚´ë¶€ì˜ í•™ìŠµ í´ë” íƒìƒ‰
            for item in os.listdir(category_path):
                item_path = os.path.join(category_path, item)
                if not os.path.isdir(item_path):
                    continue

                # íŒ¨í„´: 01_alice, 5_alic3 woman ë“±
                # ì–¸ë”ìŠ¤ì½”ì–´ ë˜ëŠ” ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ (DreamBooth í˜•ì‹ ì§€ì›)
                parts = item.split('_', 1)
                if len(parts) == 2 and parts[0].isdigit():
                    order = int(parts[0])
                    name = parts[1]
                    folders.append({
                        'order': order,
                        'name': name,
                        'path': item_path,
                        'folder': item,
                        'category': category  # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                    })

        if not folders:
            print(f"âŒ í•™ìŠµ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print(f"   ê²½ë¡œ: {train_dir}")
            print(f"   ì°¾ëŠ” ìœ„ì¹˜: {train_dir}/mainchar/, {train_dir}/background/")
            print(f"   íŒ¨í„´: 01_name, 02_name, 5_name class, ...")
            return []

        # ìˆœì„œëŒ€ë¡œ ì •ë ¬
        folders.sort(key=lambda x: x['order'])

        print(f"âœ… ë°œê²¬ëœ í•™ìŠµ í´ë”: {len(folders)}ê°œ")
        for f in folders:
            print(f"   [{f['category']}] {f['order']:02d}_{f['name']}")

        return folders

    def count_images(self, folder_path):
        """í´ë” ë‚´ ì´ë¯¸ì§€ ê°œìˆ˜ ì„¸ê¸°"""
        count = 0
        for file in os.listdir(folder_path):
            if Path(file).suffix.lower() in self.image_extensions:
                count += 1
        return count

    def calculate_training_params(self, image_count):
        """ì´ë¯¸ì§€ ìˆ˜ì— ë”°ë¥¸ ìµœì  í•™ìŠµ íŒŒë¼ë¯¸í„° ê³„ì‚°"""
        batch_size = self.config.batch_size
        target_steps = self.config.target_steps

        # ê°•ì œ ë°˜ë³µ íšŸìˆ˜ê°€ ì§€ì •ë˜ë©´ ì‚¬ìš©
        if self.config.force_repeats is not None:
            optimal_repeats = self.config.force_repeats
        else:
            # ì´ë¯¸ì§€ ìˆ˜ì— ë”°ë¥¸ ìë™ ê³„ì‚°
            if image_count < 20:
                optimal_repeats = max(80, min(200, target_steps // (image_count * 10)))
            elif image_count < 50:
                optimal_repeats = max(30, min(80, target_steps // (image_count * 10)))
            elif image_count < 100:
                optimal_repeats = max(15, min(30, target_steps // (image_count * 10)))
            else:
                optimal_repeats = max(5, min(20, target_steps // (image_count * 10)))

        # Epochs ê³„ì‚°
        images_per_epoch = image_count * optimal_repeats
        steps_per_epoch = images_per_epoch // batch_size
        actual_epochs = max(1, round(target_steps / steps_per_epoch))
        actual_epochs = min(max(actual_epochs, 5), 30)
        actual_total_steps = actual_epochs * steps_per_epoch

        return {
            'repeats': optimal_repeats,
            'epochs': actual_epochs,
            'steps_per_epoch': steps_per_epoch,
            'total_steps': actual_total_steps
        }

    def train_single_lora(self, folder_info):
        """ë‹¨ì¼ LoRA í•™ìŠµ"""
        name = folder_info['name']
        folder = folder_info['folder']
        folder_path = folder_info['path']
        category = folder_info['category']  # ì¶”ê°€!

        # ì´ë¯¸ì§€ ê°œìˆ˜ ê³„ì‚°
        num_images = self.count_images(folder_path)
        if num_images == 0:
            print(f"âŒ {name}: ì´ë¯¸ì§€ ì—†ìŒ")
            return False

        # í•™ìŠµ íŒŒë¼ë¯¸í„° ìë™ ê³„ì‚°
        params = self.calculate_training_params(num_images)
        repeats = params['repeats']
        epochs = params['epochs']

        print(f"\n{'=' * 70}")
        print(f"ğŸ¯ Training LoRA: {name}")
        print(f"{'=' * 70}")
        print(f"ğŸ“Š Training Configuration")
        print(f"{'-' * 70}")
        print(f"  Category:        {category}")
        print(f"  Folder:          {folder}")
        print(f"  Images:          {num_images}")
        print(f"  Repeats:         {repeats} (auto)")
        print(f"  Epochs:          {epochs}")
        print(f"  Total steps:     {num_images * repeats * epochs}")
        print(f"{'-' * 70}")

        # train_data_dirëŠ” ì¹´í…Œê³ ë¦¬ í´ë” (01_alic3 womanì˜ ë¶€ëª¨)
        train_data_dir = os.path.join(self.config.train_dir, category)

        cmd = [
            'accelerate', 'launch',
            '--num_cpu_threads_per_process', '1',
            '--mixed_precision', self.config.precision,  # LoRATrainer -> TrainingConfig ì°¸ì¡°
            'sdxl_train_network.py',
            f'--config_file={self.config.config_file}',  # ë™ì¼í•˜ê²Œ ì°¸ì¡°
            f'--train_data_dir={train_data_dir}',
            f'--output_name={name.replace(" ", "_")}',
            f'--max_train_epochs={epochs}',
            f'--dataset_repeats={repeats}'
        ]

        # ì‹¤í–‰
        try:
            env = os.environ.copy()
            env['CUDA_VISIBLE_DEVICES'] = str(self.config.gpu_id)

            print(f"ğŸš€ Starting training...\n")
            print(f"ğŸ“‚ Train dir: {train_data_dir}")
            result = subprocess.run(cmd, env=env, check=True)

            print(f"\nâœ… {name} í•™ìŠµ ì™„ë£Œ!")
            print(f"{'=' * 70}\n")
            return True

        except subprocess.CalledProcessError as e:
            print(f"\nâŒ {name} í•™ìŠµ ì‹¤íŒ¨: {e}")
            print(f"{'=' * 70}\n")
            return False
        except KeyboardInterrupt:
            print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            return False

    def run_batch_training(self):
        """ì¼ê´„ í•™ìŠµ ì‹¤í–‰"""
        folders = self.find_training_folders()

        if not folders:
            print("âŒ í•™ìŠµ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print(f"   ê²½ë¡œ: {self.config.train_dir}")
            print(f"   íŒ¨í„´: 01_name, 02_name, ...")
            return

        print(f"\n{'=' * 70}")
        print(f"ğŸš€ SDXL LoRA Batch Training")
        print(f"{'=' * 70}")
        print(f"ğŸ“ í•™ìŠµ í´ë”: {self.config.train_dir}")
        print(f"ğŸ’¾ ì¶œë ¥ í´ë”: {self.config.output_dir}")
        print(f"ğŸ–¥ï¸  GPU: {self.config.gpu_id} ({self.config.vram_size}GB)")
        print(f"âš¡ Precision: {self.config.precision}")
        print(f"ğŸ“‹ Config: {self.config.config_file}")
        print(f"\në°œê²¬ëœ í•™ìŠµ í´ë”: {len(folders)}ê°œ")
        print(f"{'-' * 70}")
        for f in folders:
            img_count = self.count_images(f['path'])
            print(f"  {f['order']:02d}. {f['name']:20s} ({img_count} images)")
        print(f"{'=' * 70}\n")

        # ì‚¬ìš©ì í™•ì¸
        try:
            response = input("í•™ìŠµì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() not in ['y', 'yes']:
                print("âŒ í•™ìŠµ ì·¨ì†Œë¨")
                return
        except KeyboardInterrupt:
            print("\nâŒ í•™ìŠµ ì·¨ì†Œë¨")
            return

        # í•™ìŠµ ì‹¤í–‰
        results = []
        for i, folder in enumerate(folders, 1):
            print(f"\n[{i}/{len(folders)}] Processing: {folder['name']}...")
            success = self.train_single_lora(folder)
            results.append({
                'name': folder['name'],
                'success': success
            })

            # ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰í• ì§€ ë¬¼ì–´ë´„
            if not success:
                try:
                    response = input("â“ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ")
                    if response.lower() in ['n', 'no']:
                        print("âš ï¸ ë‚˜ë¨¸ì§€ í•™ìŠµ ê±´ë„ˆëœ€")
                        break
                except KeyboardInterrupt:
                    print("\nâš ï¸ ë‚˜ë¨¸ì§€ í•™ìŠµ ê±´ë„ˆëœ€")
                    break

        # ê²°ê³¼ ìš”ì•½
        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š Training Summary")
        print(f"{'=' * 70}")
        success_count = sum(1 for r in results if r['success'])
        fail_count = len(results) - success_count

        for r in results:
            status = "âœ…" if r['success'] else "âŒ"
            print(f"{status} {r['name']}")

        print(f"{'-' * 70}")
        print(f"âœ… ì„±ê³µ: {success_count}/{len(results)}")
        if fail_count > 0:
            print(f"âŒ ì‹¤íŒ¨: {fail_count}/{len(results)}")
        print(f"{'=' * 70}\n")


def main():
    parser = argparse.ArgumentParser(
        description="SDXL LoRA ì¼ê´„ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python train_batch.py
  python train_batch.py config-16g.toml
  python train_batch.py config-24g.toml 0 15

í´ë” êµ¬ì¡°:
  training/
  â”œâ”€â”€ 01_alice/
  â”‚   â””â”€â”€ *.jpg
  â”œâ”€â”€ 02_bob/
  â”‚   â””â”€â”€ *.jpg
  â””â”€â”€ 03_background/
      â””â”€â”€ *.jpg
        """
    )

    parser.add_argument(
        "config",
        nargs="?",
        default="config-24g.toml",
        help="Config íŒŒì¼ (ê¸°ë³¸: config-24g.toml)"
    )

    parser.add_argument(
        "gpu_id",
        nargs="?",
        type=int,
        default=0,
        help="GPU ID (ê¸°ë³¸: 0)"
    )

    parser.add_argument(
        "repeats",
        nargs="?",
        type=int,
        default=None,
        help="ê°•ì œ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: ìë™ ê³„ì‚°)"
    )

    args = parser.parse_args()

    try:
        # ì„¤ì • ë¡œë“œ
        training_config = TrainingConfig(
            config_file=args.config,
            gpu_id=args.gpu_id,
            force_repeats=args.repeats
        )

        # í•™ìŠµ ì‹¤í–‰
        trainer = LoRATrainer(training_config)
        trainer.run_batch_training()

    except KeyboardInterrupt:
        print("\n\nâš ï¸ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()