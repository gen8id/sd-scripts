"""
ë…ë¦½ ì‹¤í–‰í˜• BLIP + WD14 í•˜ì´ë¸Œë¦¬ë“œ ìº¡ì…˜ ìƒì„±ê¸°
ì‹¤ì‚¬ LoRA í•™ìŠµì„ ìœ„í•œ í†µí•© ìº¡ì…˜ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ì„¤ì¹˜ í•„ìš”:
pip install transformers pillow torch torchvision onnxruntime-gpu huggingface_hub
"""

import os
import sys
from pathlib import Path
from tqdm import tqdm
import argparse
from PIL import Image
import torch


# ==============================
# âš™ï¸ ì„¤ì • (ìˆ˜ì • ê°€ëŠ¥)
# ==============================

class Config:
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    DATASET_DIRS = [
        "./dataset/mainchar",  # ë©”ì¸ ìºë¦­í„°
        "./dataset/bg",        # ë°°ê²½/ë³´ì¡°
    ]
    
    # ëª¨ë¸ ì„¤ì •
    BLIP_MODEL = "Salesforce/blip-image-captioning-large"
    WD14_MODEL = "SmilingWolf/wd-v1-4-moat-tagger-v2"
    
    # WD14 ì„ê³„ê°’
    WD14_GENERAL_THRESHOLD = 0.35
    WD14_CHARACTER_THRESHOLD = 0.85
    
    # BLIP ì„¤ì •
    BLIP_MAX_LENGTH = 75
    BLIP_NUM_BEAMS = 1  # 1=greedy, >1=beam search
    
    # ì œê±°í•  WD14 ë©”íƒ€ íƒœê·¸
    REMOVE_TAGS = [
        # ë©”íƒ€ íƒœê·¸
        "1girl", "1boy", "solo", "2girls", "3girls", "multiple girls",
        "looking at viewer", "facing viewer", "solo focus",
        # ë°°ê²½
        "simple background", "white background", "grey background",
        "transparent background", "gradient background",
        # í’ˆì§ˆ/ë©”íƒ€ë°ì´í„°
        "highres", "absurdres", "lowres", "bad anatomy",
        "signature", "watermark", "artist name", "dated",
        "commentary", "username",
        # Danbooru ë©”íƒ€
        "rating:safe", "rating:questionable", "rating:explicit",
        "safe", "questionable", "explicit",
    ]
    
    # ì¶œë ¥ ì„¤ì •
    OUTPUT_ENCODING = "utf-8"
    OVERWRITE_EXISTING = False
    CREATE_BACKUP = True
    
    # ë””ë°”ì´ìŠ¤
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ìº¡ì…˜ í¬ë§·
    # "blip_first": BLIP ë¬¸ì¥ì´ ë¨¼ì €
    # "tags_first": WD14 íƒœê·¸ê°€ ë¨¼ì €
    CAPTION_FORMAT = "blip_first"


# ==============================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==============================

def normalize_tags(tags_str):
    """íƒœê·¸ ì •ê·œí™”: ì†Œë¬¸ì, ê³µë°± ì •ë¦¬, ì¤‘ë³µ ì œê±°"""
    if not tags_str:
        return []
    tags = [tag.strip().lower() for tag in tags_str.split(',')]
    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
    seen = set()
    unique_tags = []
    for tag in tags:
        if tag and tag not in seen:
            seen.add(tag)
            unique_tags.append(tag)
    return unique_tags


def remove_unwanted_tags(tags_list, remove_list):
    """ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°"""
    remove_set = set(tag.lower() for tag in remove_list)
    return [tag for tag in tags_list if tag not in remove_set]


def merge_captions(blip_caption, wd14_tags, remove_tags, format_type="blip_first"):
    """
    BLIP ìº¡ì…˜ê³¼ WD14 íƒœê·¸ ë³‘í•©
    """
    # BLIP ì •ê·œí™”
    blip_normalized = blip_caption.strip().lower() if blip_caption else ""
    
    # WD14 íƒœê·¸ ì •ê·œí™” ë° í•„í„°ë§
    wd14_normalized = normalize_tags(wd14_tags)
    wd14_filtered = remove_unwanted_tags(wd14_normalized, remove_tags)
    
    # BLIP ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ (ì¤‘ë³µ ì œê±°ìš©)
    blip_words = set(blip_normalized.replace(',', ' ').split()) if blip_normalized else set()
    
    # WD14ì—ì„œ BLIP ì¤‘ë³µ ì œê±°
    wd14_deduped = []
    for tag in wd14_filtered:
        # ë‹¨ìˆœ ì¤‘ë³µ ì²´í¬ (ì„ íƒì ìœ¼ë¡œ ë¹„í™œì„±í™” ê°€ëŠ¥)
        tag_words = set(tag.replace('_', ' ').split())
        if not tag_words.intersection(blip_words):
            wd14_deduped.append(tag)
    
    # ìµœì¢… ë³‘í•©
    if format_type == "blip_first":
        # BLIP ë¬¸ì¥ + WD14 íƒœê·¸
        if blip_normalized and wd14_deduped:
            merged = f"{blip_normalized}, {', '.join(wd14_deduped)}"
        elif blip_normalized:
            merged = blip_normalized
        elif wd14_deduped:
            merged = ', '.join(wd14_deduped)
        else:
            merged = ""
    else:
        # WD14 íƒœê·¸ + BLIP ë¬¸ì¥
        if wd14_deduped and blip_normalized:
            merged = f"{', '.join(wd14_deduped)}, {blip_normalized}"
        elif wd14_deduped:
            merged = ', '.join(wd14_deduped)
        elif blip_normalized:
            merged = blip_normalized
        else:
            merged = ""
    
    return merged


# ==============================
# ğŸ¨ BLIP ìº¡ì…˜ ìƒì„±
# ==============================

class BLIPCaptioner:
    def __init__(self, model_name, device, max_length=75, num_beams=1):
        from transformers import BlipProcessor, BlipForConditionalGeneration
        
        print(f"  â†’ BLIP ëª¨ë¸ ë¡œë”©... ({model_name})")
        self.processor = BlipProcessor.from_pretrained(model_name)
        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)
        self.model.eval()
        self.device = device
        self.max_length = max_length
        self.num_beams = num_beams
    
    def generate(self, image_path):
        try:
            image = Image.open(image_path).convert("RGB")
            inputs = self.processor(image, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=self.max_length,
                    num_beams=self.num_beams,
                )
            
            caption = self.processor.decode(outputs[0], skip_special_tokens=True)
            return caption.strip()
        
        except Exception as e:
            print(f"âš ï¸ BLIP ì‹¤íŒ¨ ({Path(image_path).name}): {e}")
            return ""


# ==============================
# ğŸ·ï¸ WD14 íƒœê·¸ ìƒì„±
# ==============================

class WD14Tagger:
    def __init__(self, model_name, device, general_thresh=0.35, character_thresh=0.85):
        import onnxruntime as ort
        from huggingface_hub import hf_hub_download
        
        print(f"  â†’ WD14 ëª¨ë¸ ë¡œë”©... ({model_name})")
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        model_path = hf_hub_download(model_name, filename="model.onnx")
        tags_path = hf_hub_download(model_name, filename="selected_tags.csv")
        
        # ONNX ì„¸ì…˜
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == "cuda" else ['CPUExecutionProvider']
        self.session = ort.InferenceSession(model_path, providers=providers)
        
        # íƒœê·¸ ë¡œë“œ
        import pandas as pd
        self.tags_df = pd.read_csv(tags_path)
        self.general_thresh = general_thresh
        self.character_thresh = character_thresh
    
    def generate(self, image_path):
        try:
            import numpy as np
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            image = Image.open(image_path).convert("RGB")
            image = image.resize((448, 448))
            image_array = np.array(image).astype(np.float32) / 255.0
            image_array = np.expand_dims(image_array, axis=0)
            
            # ì¶”ë¡ 
            input_name = self.session.get_inputs()[0].name
            output = self.session.run(None, {input_name: image_array})[0]
            
            # íƒœê·¸ í•„í„°ë§
            tags = []
            for i, score in enumerate(output[0]):
                tag_type = self.tags_df.iloc[i]['category']
                threshold = self.character_thresh if tag_type == 4 else self.general_thresh
                
                if score >= threshold:
                    tag_name = self.tags_df.iloc[i]['name'].replace('_', ' ')
                    tags.append(tag_name)
            
            return ', '.join(tags)
        
        except Exception as e:
            print(f"âš ï¸ WD14 ì‹¤íŒ¨ ({Path(image_path).name}): {e}")
            return ""


# ==============================
# ğŸ“ íŒŒì¼ ì²˜ë¦¬
# ==============================

def get_image_files(directory):
    """ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°"""
    extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
    image_files = []
    
    dir_path = Path(directory)
    for ext in extensions:
        image_files.extend(dir_path.glob(f"*{ext}"))
        image_files.extend(dir_path.glob(f"*{ext.upper()}"))
    
    return sorted(image_files)


def create_backup(caption_path):
    """ë°±ì—… ìƒì„±"""
    if caption_path.exists():
        backup_dir = caption_path.parent / "caption_backup"
        backup_dir.mkdir(exist_ok=True)
        
        import shutil
        backup_path = backup_dir / caption_path.name
        shutil.copy2(caption_path, backup_path)


# ==============================
# ğŸš€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤
# ==============================

def process_directory(directory, blip_captioner, wd14_tagger, config):
    """ë””ë ‰í† ë¦¬ ì²˜ë¦¬"""
    print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {directory}")
    
    image_files = get_image_files(directory)
    
    if not image_files:
        print(f"âš ï¸ ì´ë¯¸ì§€ ì—†ìŒ: {directory}")
        return 0
    
    print(f"ğŸ“¸ {len(image_files)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
    
    success_count = 0
    skip_count = 0
    
    for image_path in tqdm(image_files, desc="ìº¡ì…˜ ìƒì„±"):
        caption_path = image_path.with_suffix('.txt')
        
        # ê¸°ì¡´ íŒŒì¼ í™•ì¸
        if caption_path.exists() and not config.OVERWRITE_EXISTING:
            skip_count += 1
            continue
        
        # ë°±ì—…
        if config.CREATE_BACKUP and caption_path.exists():
            create_backup(caption_path)
        
        try:
            # BLIP ìƒì„±
            blip_caption = blip_captioner.generate(image_path)
            
            # WD14 ìƒì„±
            wd14_tags = wd14_tagger.generate(image_path)
            
            # ë³‘í•©
            merged = merge_captions(
                blip_caption, wd14_tags, 
                config.REMOVE_TAGS, 
                config.CAPTION_FORMAT
            )
            
            # ì €ì¥
            if merged:
                with open(caption_path, 'w', encoding=config.OUTPUT_ENCODING) as f:
                    f.write(merged)
                success_count += 1
            else:
                print(f"âš ï¸ ë¹ˆ ìº¡ì…˜: {image_path.name}")
        
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨ ({image_path.name}): {e}")
            continue
    
    print(f"âœ… ì™„ë£Œ: {success_count}ê°œ ìƒì„±, {skip_count}ê°œ ìŠ¤í‚µ")
    return success_count


def main():
    parser = argparse.ArgumentParser(description="BLIP + WD14 í•˜ì´ë¸Œë¦¬ë“œ ìº¡ì…˜")
    parser.add_argument("--dirs", nargs="+", help="ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬")
    parser.add_argument("--overwrite", action="store_true", help="ë®ì–´ì“°ê¸°")
    parser.add_argument("--device", default=None, help="cuda/cpu")
    parser.add_argument("--format", choices=["blip_first", "tags_first"], help="ìº¡ì…˜ í¬ë§·")
    
    args = parser.parse_args()
    
    config = Config()
    if args.dirs:
        config.DATASET_DIRS = args.dirs
    if args.overwrite:
        config.OVERWRITE_EXISTING = True
    if args.device:
        config.DEVICE = args.device
    if args.format:
        config.CAPTION_FORMAT = args.format
    
    print("=" * 60)
    print("ğŸ¨ BLIP + WD14 í•˜ì´ë¸Œë¦¬ë“œ ìº¡ì…˜ ìƒì„±ê¸°")
    print("=" * 60)
    print(f"ğŸ“ ëŒ€ìƒ: {config.DATASET_DIRS}")
    print(f"ğŸ’¾ ë®ì–´ì“°ê¸°: {config.OVERWRITE_EXISTING}")
    print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {config.DEVICE}")
    print(f"ğŸ“ í¬ë§·: {config.CAPTION_FORMAT}")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    try:
        blip_captioner = BLIPCaptioner(
            config.BLIP_MODEL,
            config.DEVICE,
            config.BLIP_MAX_LENGTH,
            config.BLIP_NUM_BEAMS
        )
        
        wd14_tagger = WD14Tagger(
            config.WD14_MODEL,