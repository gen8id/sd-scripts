#!/usr/bin/env python3
"""
SDXL LoRA Îã®Ïùº ÌïôÏäµ Ïä§ÌÅ¨Î¶ΩÌä∏ (Í≥†Í∏â ÏÇ¨Ïö©ÏûêÏö©)
- ÌäπÏ†ï Ìè¥ÎçîÎßå ÏÑ†ÌÉù ÌïôÏäµ
- ÏÑ∏Î∞ÄÌïú ÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ï†ï Í∞ÄÎä•
- Config Ïò§Î≤ÑÎùºÏù¥Îìú
"""

import os
import sys
import logging
import re
import toml
import subprocess
import argparse
from pathlib import Path
from run_train_auto import TrainingConfig, LoRATrainer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # ÌÑ∞ÎØ∏ÎÑêÎ°ú
        logging.FileHandler("/app/sdxl_train_captioner/logs/train_single_debug.log")  # ÌååÏùºÎ°ú
    ]
)

def get_vram_size(gpu_id=0):
    """NVIDIA GPU VRAM ÌÅ¨Í∏∞ Í∞êÏßÄ (GB)"""
    try:
        cmd = [
            "nvidia-smi",
            "--query-gpu=memory.total",
            "--format=csv,noheader,nounits",
            "-i", str(gpu_id)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        vram_mb = int(result.stdout.strip())
        return vram_mb // 1024
    except:
        return 24  # Í∏∞Î≥∏Í∞í


def count_images(folder_path):
    """Ìè¥Îçî ÎÇ¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò ÏÑ∏Í∏∞ (1Îã®Í≥Ñ ÌïòÏúÑ Ìè¥ÎçîÍπåÏßÄ Ìè¨Ìï®)"""
    extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
    count = 0

    folder_path = Path(folder_path)

    # ÏÉÅÏúÑ Ìè¥Îçî ÏïàÏùò ÏßÅÏ†ë Ïù¥ÎØ∏ÏßÄ
    for file in folder_path.iterdir():
        if file.is_file() and file.suffix.lower() in extensions:
            count += 1

    # Î∞îÎ°ú ÏïÑÎûò 1Îã®Í≥Ñ ÌïòÏúÑ Ìè¥Îçî Ïù¥ÎØ∏ÏßÄ
    for subfolder in folder_path.iterdir():
        if subfolder.is_dir():
            for file in subfolder.iterdir():
                if file.is_file() and file.suffix.lower() in extensions:
                    count += 1

    return count


def calculate_auto_params(image_count, vram_size, batch_size=1):
    """Ïù¥ÎØ∏ÏßÄ Ïàò Í∏∞Î∞ò ÏûêÎèô ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞"""
    target_steps = 1800 if vram_size >= 20 else 1500

    # Repeats Í≥ÑÏÇ∞
    if image_count < 20:
        repeats = max(80, min(200, target_steps // (image_count * 10)))
    elif image_count < 50:
        repeats = max(30, min(80, target_steps // (image_count * 10)))
    elif image_count < 100:
        repeats = max(15, min(30, target_steps // (image_count * 10)))
    else:
        repeats = max(5, min(20, target_steps // (image_count * 10)))

    # Epochs Í≥ÑÏÇ∞
    images_per_epoch = image_count * repeats
    steps_per_epoch = images_per_epoch // batch_size
    epochs = max(1, round(target_steps / steps_per_epoch))
    epochs = min(max(epochs, 5), 30)

    return {
        'repeats': repeats,
        'epochs': epochs,
        'steps_per_epoch': steps_per_epoch,
        'total_steps': epochs * steps_per_epoch
    }


def main():
    parser = argparse.ArgumentParser(
        description="SDXL LoRA Îã®Ïùº ÌïôÏäµ (Í≥†Í∏â ÏÑ§Ï†ï)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ÏÇ¨Ïö© ÏòàÏãú:
  # Í∏∞Î≥∏ (ÏûêÎèô Í≥ÑÏÇ∞)
  python run_train_single.py --folder ../dataset/training/01_alice

  # ÏàòÎèô ÌååÎùºÎØ∏ÌÑ∞ ÏßÄÏ†ï
  python run_train_single.py --folder ../dataset/training/01_alice --epochs 20 --repeats 30

  # Learning rate Ï°∞Ï†ï
  python run_train_single.py --folder ../dataset/training/01_alice --lr 0.0002

  # Network dim Î≥ÄÍ≤Ω
  python run_train_single.py --folder ../dataset/training/01_alice --dim 64 --alpha 32

  # Ïù¥Ïñ¥ÏÑú ÌïôÏäµ (Resume)
  python run_train_single.py --folder ../dataset/training/01_alice --resume ../output_models/alice-epoch-010.safetensors --epochs 30

  # Ï†ÑÏ≤¥ Ïª§Ïä§ÌÖÄ
  python run_train_single.py \
    --folder ../dataset/training/01_alice \
    --output alice_v2 \
    --config config-24g.toml \
    --gpu 0 \
    --epochs 25 \
    --repeats 40 \
    --lr 0.00015 \
    --dim 64 \
    --alpha 32 \
    --batch-size 2
        """
    )

    # ÌïÑÏàò Ïù∏Ïûê
    parser.add_argument(
        "--folder",
        required=True,
        help="ÌïôÏäµÌï† Ìè¥Îçî Í≤ΩÎ°ú (Ïòà: ../dataset/training/01_alice)"
    )

    # Í∏∞Î≥∏ ÏÑ§Ï†ï
    parser.add_argument(
        "--config",
        default="config-24g.toml",
        help="Config ÌååÏùº (Í∏∞Î≥∏: config-24g.toml)"
    )

    parser.add_argument(
        "--output",
        help="Ï∂úÎ†• LoRA Ïù¥Î¶Ñ (Í∏∞Î≥∏: Ìè¥ÎçîÎ™ÖÏóêÏÑú Ï∂îÏ∂ú)"
    )

    parser.add_argument(
        "--gpu",
        type=int,
        default=0,
        help="GPU ID (Í∏∞Î≥∏: 0)"
    )

    # ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞
    parser.add_argument(
        "--epochs",
        type=int,
        help="Ï¥ù Epoch Ïàò (Í∏∞Î≥∏: ÏûêÎèô Í≥ÑÏÇ∞)"
    )

    parser.add_argument(
        "--repeats",
        type=int,
        help="Ïù¥ÎØ∏ÏßÄ Î∞òÎ≥µ ÌöüÏàò (Í∏∞Î≥∏: ÏûêÎèô Í≥ÑÏÇ∞)"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        help="Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à (Í∏∞Î≥∏: config Í∞í)"
    )

    parser.add_argument(
        "--lr",
        type=float,
        help="Learning rate (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 1e-4)"
    )

    parser.add_argument(
        "--dim",
        type=int,
        help="Network dimension (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 32)"
    )

    parser.add_argument(
        "--alpha",
        type=int,
        help="Network alpha (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 16)"
    )

    parser.add_argument(
        "--resolution",
        help="Ìï¥ÏÉÅÎèÑ (Ïòà: 1024,1024 ÎòêÎäî 768,768)"
    )

    parser.add_argument(
        "--save-every",
        type=int,
        help="N epochÎßàÎã§ Ï†ÄÏû• (Í∏∞Î≥∏: config Í∞í)"
    )

    # Í≥†Í∏â ÏòµÏÖò
    parser.add_argument(
        "--optimizer",
        help="Optimizer (Ïòà: AdamW8bit, Lion, Prodigy)"
    )

    parser.add_argument(
        "--scheduler",
        help="LR Scheduler (Ïòà: cosine, constant, polynomial)"
    )

    parser.add_argument(
        "--no-auto",
        action="store_true",
        help="ÏûêÎèô Í≥ÑÏÇ∞ ÎπÑÌôúÏÑ±Ìôî (epochs/repeats ÏàòÎèô ÏßÄÏ†ï ÌïÑÏàò)"
    )

    # Resume ÏòµÏÖò
    parser.add_argument(
        "--resume",
        help="Ïù¥Ïñ¥ÏÑú ÌïôÏäµÌï† LoRA ÌååÏùº Í≤ΩÎ°ú (Ïòà: ../output_models/alice-epoch-010.safetensors)"
    )

    args = parser.parse_args()

    # Ìè¥Îçî ÌôïÏù∏
    folder_path = Path(args.folder)
    if not folder_path.exists() or not folder_path.is_dir():
        print(f"‚ùå Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {folder_path}")
        sys.exit(1)

    # Ïù¥ÎØ∏ÏßÄ Í∞úÏàò
    image_count = count_images(folder_path)
    if image_count == 0:
        print(f"‚ùå Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§: {folder_path}")
        sys.exit(1)

    # VRAM Í∞êÏßÄ
    vram_size = get_vram_size(args.gpu)

    # Config ÏûêÎèô ÏÑ†ÌÉù
    if vram_size >= 20:
        precision = "bf16"
        config_file = args.config if args.config else "config-24g.toml"
    else:
        precision = "fp16"
        config_file = args.config if args.config else "config-16g.toml"
        print(f"‚ö†Ô∏è VRAM {vram_size}GB < 20GB, fp16 Î™®ÎìúÎ°ú Ï†ÑÌôò")

    # Config Î°úÎìú
    if not os.path.exists(config_file):
        print(f"‚ùå Config ÌååÏùº ÏóÜÏùå: {config_file}")
        sys.exit(1)

    with open(config_file, 'r', encoding='utf-8') as f:
        config = toml.load(f)

    batch_size = args.batch_size or config['training'].get('batch_size', 1)

    # Ï∂úÎ†• Ïù¥Î¶Ñ Í≤∞Ï†ï
    if args.output:
        output_name = args.output
    else:
        # Ìè¥ÎçîÎ™ÖÏóêÏÑú Ï∂îÏ∂ú (01_alice_woman ‚Üí alice)
        folder_name = folder_path.name
        parts = folder_name.split('_', 1)
        if len(parts) == 2 and parts[0].isdigit():
            base_name = parts[1]
        else:
            base_name = folder_name

        # ÌÅ¥ÎûòÏä§ Ï†ëÎØ∏ÏÇ¨ Ï†úÍ±∞ (_woman, _man Îì±)
        base_name = re.sub(r'_[a-zA-Z0-9]+$', '', base_name)
        output_name = base_name

    # üìå Resume Ïãú Ï∂úÎ†• Ïù¥Î¶Ñ Ïû¨Í≤∞Ï†ï Î°úÏßÅ Î≥¥ÏôÑ
    # ÏöîÏ≤≠ Î™ÖÎ†πÏñ¥ÏóêÏÑúÎäî output_nameÏù¥ Îî∞Î°ú ÏßÄÏ†ïÎêòÏßÄ ÏïäÏïÑ, Ìè¥ÎçîÎ™ÖÏóêÏÑú Ï∂îÏ∂úÎêú Ïù¥Î¶ÑÏù¥ ÏÇ¨Ïö©Îê©ÎãàÎã§.
    # ÎßåÏïΩ --resumeÏóê ÏûàÎäî ÌååÏùºÎ™Ö Í∏∞Î∞òÏúºÎ°ú output_nameÏùÑ ÏßÄÏ†ïÌïòÍ≥† Ïã∂Îã§Î©¥ ÏïÑÎûò Î°úÏßÅÏùÑ ÏÇ¨Ïö©Ìï¥Ïïº ÌïòÏßÄÎßå,
    # ÌòÑÏû¨Îäî Ìè¥ÎçîÎ™Ö Í∏∞Î∞òÏúºÎ°ú ÏßÑÌñâÌï©ÎãàÎã§.

    # ÌååÎùºÎØ∏ÌÑ∞ Í≤∞Ï†ï
    if args.no_auto:
        # ÏàòÎèô Î™®Îìú
        if not args.epochs or not args.repeats:
            print("‚ùå --no-auto ÏÇ¨Ïö© Ïãú --epochsÏôÄ --repeats ÌïÑÏàòÏûÖÎãàÎã§")
            sys.exit(1)
        epochs = args.epochs
        repeats = args.repeats
        steps_per_epoch = (image_count * repeats) // batch_size
        total_steps = epochs * steps_per_epoch
    else:
        # ÏûêÎèô Í≥ÑÏÇ∞ (Ïò§Î≤ÑÎùºÏù¥Îìú Í∞ÄÎä•)
        auto_params = calculate_auto_params(image_count, vram_size, batch_size)
        epochs = args.epochs or auto_params['epochs']
        repeats = args.repeats or auto_params['repeats']
        steps_per_epoch = (image_count * repeats) // batch_size
        total_steps = epochs * steps_per_epoch

    # ÌïôÏäµ Ï†ïÎ≥¥ Ï∂úÎ†•
    print(f"\n{'=' * 70}")
    print(f"üéØ SDXL LoRA Training - Single Mode")
    print(f"{'=' * 70}")
    print(f"üìÅ Folder:           {folder_path}")
    print(f"üíæ Output:           {output_name}.safetensors")
    print(f"üìã Config:           {config_file}")
    print(f"üñ•Ô∏è  GPU:            {args.gpu} ({vram_size}GB VRAM)")
    print(f"‚ö° Precision:        {precision}")

    # Resume Ï†ïÎ≥¥
    if args.resume:
        # resume ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏùÑ Í≤ΩÏö∞Î•º ÎåÄÎπÑÌïòÏó¨ Ï†àÎåÄ Í≤ΩÎ°úÎ°ú Î≥ÄÌôòÌïòÏó¨ ÌôïÏù∏
        resume_path = Path(args.resume)
        if not resume_path.is_absolute():
            # ÏÉÅÎåÄ Í≤ΩÎ°úÏù∏ Í≤ΩÏö∞ ÌòÑÏû¨ Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ ÏúÑÏπò Í∏∞Ï§ÄÏúºÎ°ú Ï≤òÎ¶¨
            resume_path = Path(os.getcwd()) / resume_path

        if not resume_path.exists():
            print(f"‚ùå Resume ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {resume_path}")
            sys.exit(1)
        print(f"üîÑ Resume from:      {resume_path}")

    print(f"{'-' * 70}")
    print(f"üìä Training Parameters")
    print(f"{'-' * 70}")
    print(f"  Images:            {image_count}")
    print(f"  Repeats:           {repeats}" + (" (manual)" if args.repeats else " (auto)"))
    print(f"  Epochs:            {epochs}" + (" (manual)" if args.epochs else " (auto)"))
    print(f"  Batch size:        {batch_size}" + (" (override)" if args.batch_size else ""))
    print(f"  Images/epoch:      {image_count * repeats}")
    print(f"  Steps/epoch:       {steps_per_epoch}")
    print(f"  Total steps:       {total_steps}")

    # Ïò§Î≤ÑÎùºÏù¥ÎìúÎêú ÌååÎùºÎØ∏ÌÑ∞ ÌëúÏãú (Ïù¥Ï†ÑÍ≥º ÎèôÏùº)
    overrides = []
    if args.lr:
        print(f"  Learning rate:     {args.lr} (override)")
        overrides.append(('lr', args.lr))
    if args.dim:
        print(f"  Network dim:       {args.dim} (override)")
        overrides.append(('dim', args.dim))
    if args.alpha:
        print(f"  Network alpha:     {args.alpha} (override)")
        overrides.append(('alpha', args.alpha))
    if args.resolution:
        print(f"  Resolution:        {args.resolution} (override)")
        overrides.append(('resolution', args.resolution))
    if args.optimizer:
        print(f"  Optimizer:         {args.optimizer} (override)")
        overrides.append(('optimizer', args.optimizer))
    if args.scheduler:
        print(f"  LR Scheduler:      {args.scheduler} (override)")
        overrides.append(('scheduler', args.scheduler))
    if args.save_every:
        print(f"  Save every:        {args.save_every} epochs (override)")
        overrides.append(('save_every', args.save_every))

    print(f"{'=' * 70}\n")

    # ÏÇ¨Ïö©Ïûê ÌôïÏù∏
    try:
        response = input("ÌïôÏäµÏùÑ ÏãúÏûëÌïòÏãúÍ≤†ÏäµÎãàÍπå? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("‚ùå ÌïôÏäµ Ï∑®ÏÜåÎê®")
            sys.exit(0)
    except KeyboardInterrupt:
        print("\n‚ùå ÌïôÏäµ Ï∑®ÏÜåÎê®")
        sys.exit(0)

    # accelerate Î™ÖÎ†πÏñ¥ Íµ¨ÏÑ±
    cmd = [
        "accelerate", "launch",
        "--num_cpu_threads_per_process", "1",
        "--mixed_precision", precision,
        "sdxl_train_network.py",
        f"--config_file={config_file}",
        f"--train_data_dir={folder_path}",
        f"--output_name={output_name}",
        f"--max_train_epochs={epochs}",
        f"--dataset_repeats={repeats}"
    ]

    # resume(Ïù¥Ïñ¥Î∞õÍ∏∞) Ï≤òÎ¶¨ (Ï§ëÎ≥µÎêú Î°úÏßÅ Ï†úÍ±∞ÌïòÍ≥†, ÌïÑÏöîÌïú Î∂ÄÎ∂ÑÎßå ÎÇ®ÍπÄ)
    if args.resume:
        resume_path = str(Path(args.resume).resolve())
        if os.path.exists(resume_path):
            # LoRA Í∞ÄÏ§ëÏπò Î°úÎìú: Kohya_SSÏóêÏÑú ÌïôÏäµ Ïû¨Í∞ú Ïãú ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÏÇ¨Ïö©
            cmd.append(f"--network_weights={resume_path}")
            print(f"\nüîÑ LoRA Í∞ÄÏ§ëÏπò Î°úÎìú (--network_weights): {resume_path}\n")
        else:
            print(f"‚ùå Resume ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {resume_path}")
            sys.exit(1)

    # Ïò§Î≤ÑÎùºÏù¥Îìú Ï∂îÍ∞Ä
    if args.batch_size:
        cmd.append(f"--train_batch_size={args.batch_size}")
    if args.lr:
        cmd.append(f"--learning_rate={args.lr}")
    if args.dim:
        cmd.append(f"--network_dim={args.dim}")
    if args.alpha:
        cmd.append(f"--network_alpha={args.alpha}")
    if args.resolution:
        cmd.append(f"--resolution={args.resolution}")

    try:
        training_config = TrainingConfig(
            config_file=args.config,
            gpu_id=args.gpu,            # argparse --gpu
            force_repeats=args.repeats, # argparse --repeats
            folder_path=folder_path,    # ÌïôÏäµÌï† Ìè¥Îçî
            output_name=output_name,    # Ï†ÄÏû•Ìï† LoRA Ïù¥Î¶Ñ
            epochs=epochs,
            batch_size=batch_size,
            lr=args.lr,
            dim=args.dim,
            alpha=args.alpha,
            resolution=args.resolution,
            resume=args.resume
        )
        # ÌïôÏäµ Ïã§Ìñâ
        trainer = LoRATrainer(training_config)
        trainer.run_batch_training()

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è ÌîÑÎ°úÍ∑∏Îû® Ï§ëÎã®Îê®")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Ïò§Î•ò Î∞úÏÉù: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()