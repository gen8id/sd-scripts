#!/usr/bin/env python3
"""
SDXL LoRA Îã®Ïùº ÌïôÏäµ Ïä§ÌÅ¨Î¶ΩÌä∏ (Í≥†Í∏â ÏÇ¨Ïö©ÏûêÏö©)
- ÌäπÏ†ï Ìè¥ÎçîÎßå ÏÑ†ÌÉù ÌïôÏäµ
- ÏÑ∏Î∞ÄÌïú ÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ï†ï Í∞ÄÎä•
- Config Ïò§Î≤ÑÎùºÏù¥Îìú
"""

import os
import sys
import logging
import re
import toml
import subprocess
import argparse
from pathlib import Path
from run_train_auto import TrainingConfig, LoRATrainer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # ÌÑ∞ÎØ∏ÎÑêÎ°ú
        logging.FileHandler("/app/sdxl_train_captioner/logs/train_single_debug.log")  # ÌååÏùºÎ°ú
    ]
)

def get_vram_size(gpu_id=0):
    """NVIDIA GPU VRAM ÌÅ¨Í∏∞ Í∞êÏßÄ (GB)"""
    try:
        cmd = [
            "nvidia-smi",
            "--query-gpu=memory.total",
            "--format=csv,noheader,nounits",
            "-i", str(gpu_id)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        vram_mb = int(result.stdout.strip())
        return vram_mb // 1024
    except:
        return 24  # Í∏∞Î≥∏Í∞í


def count_images(folder_path):
    """Ìè¥Îçî ÎÇ¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò ÏÑ∏Í∏∞ (1Îã®Í≥Ñ ÌïòÏúÑ Ìè¥ÎçîÍπåÏßÄ Ìè¨Ìï®)"""
    extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
    count = 0

    folder_path = Path(folder_path)

    # ÏÉÅÏúÑ Ìè¥Îçî ÏïàÏùò ÏßÅÏ†ë Ïù¥ÎØ∏ÏßÄ
    for file in folder_path.iterdir():
        if file.is_file() and file.suffix.lower() in extensions:
            count += 1

    # Î∞îÎ°ú ÏïÑÎûò 1Îã®Í≥Ñ ÌïòÏúÑ Ìè¥Îçî Ïù¥ÎØ∏ÏßÄ
    for subfolder in folder_path.iterdir():
        if subfolder.is_dir():
            for file in subfolder.iterdir():
                if file.is_file() and file.suffix.lower() in extensions:
                    count += 1

    return count


def calculate_auto_params(image_count, vram_size, batch_size=1):
    """Ïù¥ÎØ∏ÏßÄ Ïàò Í∏∞Î∞ò ÏûêÎèô ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞"""
    target_steps = 1800 if vram_size >= 20 else 1500

    # Repeats Í≥ÑÏÇ∞
    if image_count < 20:
        repeats = max(80, min(200, target_steps // (image_count * 10)))
    elif image_count < 50:
        repeats = max(30, min(80, target_steps // (image_count * 10)))
    elif image_count < 100:
        repeats = max(15, min(30, target_steps // (image_count * 10)))
    else:
        repeats = max(5, min(20, target_steps // (image_count * 10)))

    # Epochs Í≥ÑÏÇ∞
    images_per_epoch = image_count * repeats
    steps_per_epoch = images_per_epoch // batch_size
    epochs = max(1, round(target_steps / steps_per_epoch))
    epochs = min(max(epochs, 5), 30)

    return {
        'repeats': repeats,
        'epochs': epochs,
        'steps_per_epoch': steps_per_epoch,
        'total_steps': epochs * steps_per_epoch
    }


def main():

    parser = argparse.ArgumentParser(
        description="SDXL LoRA Îã®Ïùº ÌïôÏäµ (Í≥†Í∏â ÏÑ§Ï†ï)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""(ÏÉùÎûµ: ÏÇ¨Ïö© ÏòàÏãú ÎèôÏùº)"""
    )

    # -- Ïù∏Ïûê Ï†ïÏùò (ÏÉùÎûµ: Í∏∞Ï°¥Í≥º ÎèôÏùº) --

    # ÌïÑÏàò Ïù∏Ïûê
    parser.add_argument(
        "--folder",
        required=True,
        help="ÌïôÏäµÌï† Ìè¥Îçî Í≤ΩÎ°ú (Ïòà: ../dataset/training/01_alice)"
    )

    # Í∏∞Î≥∏ ÏÑ§Ï†ï
    parser.add_argument(
        "--config",
        default="config-24g.toml",
        help="Config ÌååÏùº (Í∏∞Î≥∏: config-24g.toml)"
    )

    parser.add_argument(
        "--output",
        help="Ï∂úÎ†• LoRA Ïù¥Î¶Ñ (Í∏∞Î≥∏: Ìè¥ÎçîÎ™ÖÏóêÏÑú Ï∂îÏ∂ú)"
    )

    parser.add_argument(
        "--gpu",
        type=int,
        default=0,
        help="GPU ID (Í∏∞Î≥∏: 0)"
    )

    # ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞
    parser.add_argument(
        "--epochs",
        type=int,
        help="Ï¥ù Epoch Ïàò (Í∏∞Î≥∏: ÏûêÎèô Í≥ÑÏÇ∞)"
    )

    parser.add_argument(
        "--repeats",
        type=int,
        help="Ïù¥ÎØ∏ÏßÄ Î∞òÎ≥µ ÌöüÏàò (Í∏∞Î≥∏: ÏûêÎèô Í≥ÑÏÇ∞)"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        help="Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à (Í∏∞Î≥∏: config Í∞í)"
    )

    parser.add_argument(
        "--lr",
        type=float,
        help="Learning rate (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 1e-4)"
    )

    parser.add_argument(
        "--dim",
        type=int,
        help="Network dimension (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 32)"
    )

    parser.add_argument(
        "--alpha",
        type=int,
        help="Network alpha (Í∏∞Î≥∏: config Í∞í, Î≥¥ÌÜµ 16)"
    )

    parser.add_argument(
        "--resolution",
        help="Ìï¥ÏÉÅÎèÑ (Ïòà: 1024,1024 ÎòêÎäî 768,768)"
    )

    parser.add_argument(
        "--save-every",
        type=int,
        help="N epochÎßàÎã§ Ï†ÄÏû• (Í∏∞Î≥∏: config Í∞í)"
    )

    # Í≥†Í∏â ÏòµÏÖò
    parser.add_argument(
        "--optimizer",
        help="Optimizer (Ïòà: AdamW8bit, Lion, Prodigy)"
    )

    parser.add_argument(
        "--scheduler",
        help="LR Scheduler (Ïòà: cosine, constant, polynomial)"
    )

    parser.add_argument(
        "--no-auto",
        action="store_true",
        help="ÏûêÎèô Í≥ÑÏÇ∞ ÎπÑÌôúÏÑ±Ìôî (epochs/repeats ÏàòÎèô ÏßÄÏ†ï ÌïÑÏàò)"
    )

    # Resume ÏòµÏÖò
    parser.add_argument(
        "--resume",
        help="Ïù¥Ïñ¥ÏÑú ÌïôÏäµÌï† LoRA ÌååÏùº Í≤ΩÎ°ú (Ïòà: ../output_models/alice-epoch-010.safetensors)"
    )
    parser.add_argument("--logging_dir", type=str, default="../logs/tensorboard",
                        help="TensorBoard Î°úÍ∑∏ Ï†ÄÏû• Ìè¥Îçî")
    parser.add_argument("--report_to", type=str, default="tensorboard",
                        help="Î°úÍ∑∏Î•º Í∏∞Î°ùÌï† ÌîåÎû´Ìèº (tensorboard)")

    args = parser.parse_args()
    print("üß© Ï†ÑÎã¨Îêú Ïù∏Ïûê:", sys.argv)
    print("üß© argparse Í≤∞Í≥º:", args)

    # ==========================================
    # 1. Í∏∞Î≥∏ Í≤ÄÏ¶ù
    # ==========================================
    folder_path = Path(args.folder)
    if not folder_path.exists() or not folder_path.is_dir():
        print(f"‚ùå Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {folder_path}")
        sys.exit(1)

    image_count = count_images(folder_path)
    if image_count == 0:
        print(f"‚ùå Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§: {folder_path}")
        sys.exit(1)

    # ==========================================
    # 2. Resume ÌååÏùº Í≤ÄÏ¶ù (ÏµúÏö∞ÏÑ† Ï≤òÎ¶¨)
    # ==========================================
    resume_path = None
    if args.resume:
        resume_path = Path(args.resume).resolve()
        if not resume_path.exists():
            print(f"‚ùå Resume ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {resume_path}")
            sys.exit(1)
        if resume_path.suffix.lower() != '.safetensors':
            print(f"‚ö†Ô∏è  Í≤ΩÍ≥†: Resume ÌååÏùºÏù¥ .safetensorsÍ∞Ä ÏïÑÎãôÎãàÎã§: {resume_path}")

    # ==========================================
    # 3. VRAM Î∞è Config ÏÑ§Ï†ï
    # ==========================================
    vram_size = get_vram_size(args.gpu)

    if vram_size >= 20:
        precision = "bf16"
        config_file = args.config if args.config else "config-24g.toml"
    else:
        precision = "fp16"
        config_file = args.config if args.config else "config-16g.toml"
        print(f"‚ö†Ô∏è VRAM {vram_size}GB < 20GB, fp16 Î™®ÎìúÎ°ú Ï†ÑÌôò")

    if not os.path.exists(config_file):
        print(f"‚ùå Config ÌååÏùº ÏóÜÏùå: {config_file}")
        sys.exit(1)

    with open(config_file, 'r', encoding='utf-8') as f:
        config = toml.load(f)

    batch_size = args.batch_size or config['training'].get('batch_size', 1)

    # ==========================================
    # 4. Ï∂úÎ†• Ïù¥Î¶Ñ Í≤∞Ï†ï
    # ==========================================
    if args.output:
        output_name = args.output
    else:
        folder_name = folder_path.name
        output_name = folder_name

    # ==========================================
    # 5. ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞ Í≤∞Ï†ï
    # ==========================================
    if args.no_auto:
        if not args.epochs or not args.repeats:
            print("‚ùå --no-auto ÏÇ¨Ïö© Ïãú --epochsÏôÄ --repeats ÌïÑÏàòÏûÖÎãàÎã§")
            sys.exit(1)
        epochs = args.epochs
        repeats = args.repeats
    else:
        auto_params = calculate_auto_params(image_count, vram_size, batch_size)
        epochs = args.epochs or auto_params['epochs']
        repeats = args.repeats or auto_params['repeats']

    steps_per_epoch = (image_count * repeats) // batch_size
    total_steps = epochs * steps_per_epoch

    # ==========================================
    # 6. ÌïôÏäµ Ï†ïÎ≥¥ Ï∂úÎ†•
    # ==========================================
    print(f"\n{'=' * 70}")
    print(f"üéØ SDXL LoRA Training - Single Mode")
    print(f"{'=' * 70}")
    print(f"üìÅ Folder:         {folder_path}")
    print(f"üíæ Output:         {output_name}.safetensors")
    print(f"üìã Config:         {config_file}")
    print(f"üñ•Ô∏è  GPU:           {args.gpu} ({vram_size}GB VRAM)")
    print(f"‚ö° Precision:       {precision}")

    if resume_path:
        print(f"üîÑ Resume from:      {resume_path}")
        print(f"   (Ïù¥Ïñ¥ÌïôÏäµ Î™®Îìú - Í∏∞Ï°¥ Í∞ÄÏ§ëÏπòÏóêÏÑú Í≥ÑÏÜç)")

    print(f"{'-' * 70}")
    print(f"üìä Training Parameters")
    print(f"{'-' * 70}")
    print(f"  Images:            {image_count}")
    print(f"  Repeats:           {repeats}" + (" (manual)" if args.repeats else " (auto)"))
    print(f"  Epochs:            {epochs}" + (" (manual)" if args.epochs else " (auto)"))
    print(f"  Batch size:        {batch_size}" + (" (override)" if args.batch_size else ""))
    print(f"  Images/epoch:      {image_count * repeats}")
    print(f"  Steps/epoch:       {steps_per_epoch}")
    print(f"  Total steps:       {total_steps}")
    print(f"{'=' * 70}\n")

    # Ïò§Î≤ÑÎùºÏù¥ÎìúÎêú ÌååÎùºÎØ∏ÌÑ∞ ÌëúÏãú (Ïù¥Ï†ÑÍ≥º ÎèôÏùº)
    overrides = []
    if args.lr:
        print(f"  Learning rate:     {args.lr} (override)")
        overrides.append(('lr', args.lr))
    if args.dim:
        print(f"  Network dim:       {args.dim} (override)")
        overrides.append(('dim', args.dim))
    if args.alpha:
        print(f"  Network alpha:     {args.alpha} (override)")
        overrides.append(('alpha', args.alpha))
    if args.resolution:
        print(f"  Resolution:        {args.resolution} (override)")
        overrides.append(('resolution', args.resolution))
    if args.optimizer:
        print(f"  Optimizer:         {args.optimizer} (override)")
        overrides.append(('optimizer', args.optimizer))
    if args.scheduler:
        print(f"  LR Scheduler:      {args.scheduler} (override)")
        overrides.append(('scheduler', args.scheduler))
    if args.save_every:
        print(f"  Save every:        {args.save_every} epochs (override)")
        overrides.append(('save_every', args.save_every))

    print(f"{'=' * 70}\n")

    # ==========================================
    # 7. ÏÇ¨Ïö©Ïûê ÌôïÏù∏
    # ==========================================
    try:
        response = input("ÌïôÏäµÏùÑ ÏãúÏûëÌïòÏãúÍ≤†ÏäµÎãàÍπå? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("‚ùå ÌïôÏäµ Ï∑®ÏÜåÎê®")
            sys.exit(0)
    except KeyboardInterrupt:
        print("\n‚ùå ÌïôÏäµ Ï∑®ÏÜåÎê®")
        sys.exit(0)

    # ==========================================
    # 8. TrainingConfig ÏÉùÏÑ±
    # ==========================================
    try:
        training_config = TrainingConfig(
            config_file=config_file,
            gpu_id=args.gpu,
            force_repeats=args.repeats
        )

        # Resume ÏÑ§Ï†ï Ï∂îÍ∞Ä
        if resume_path:
            training_config.resume = str(resume_path)

        # Ï∂îÍ∞Ä Ïò§Î≤ÑÎùºÏù¥Îìú ÏÑ§Ï†ï
        if args.batch_size:
            training_config.batch_size = args.batch_size
        if args.lr:
            training_config.learning_rate = args.lr
        if args.dim:
            training_config.network_dim = args.dim
        if args.alpha:
            training_config.network_alpha = args.alpha
        if args.resolution:
            training_config.resolution = args.resolution


    except Exception as e:
        print(f"‚ùå TrainingConfig ÏÉùÏÑ± Ïã§Ìå®: {e}")
        sys.exit(1)

    # ==========================================
    # 9. Trainer ÏÉùÏÑ± Î∞è ÌïôÏäµ Ïã§Ìñâ
    # ==========================================
    trainer = LoRATrainer(training_config)

    # folder_info ÏÉùÏÑ±
    folder_name = folder_path.name
    folder_split_len = len(folder_name.split('_'))
    parts = folder_name.split('_', 1)
    if len(parts) == 2 and parts[0].isdigit():
        order = int(parts[0])
    else:
        order = 0

    folder_info = {
        'order': order,
        'name': folder_name,
        'path': str(folder_path),
        'folder': folder_name,
        'category': folder_path.parent.name,
        'output_name': output_name,  # Ï∂úÎ†• Ïù¥Î¶Ñ Ï†ÑÎã¨
        'epochs': epochs,
        'repeats': repeats,
    }

    if folder_split_len == 3:
        folder_info['class'] = folder_name.split('_')[2]

    # ÌïôÏäµ Ïã§Ìñâ
    print("\nüöÄ ÌïôÏäµ ÏãúÏûë...\n")
    success = trainer.train_single_lora(folder_info)

    if not success:
        print("\n‚ùå ÌïôÏäµ Ïã§Ìå®")
        sys.exit(1)
    else:
        print(f"\n‚úÖ ÌïôÏäµ ÏôÑÎ£å: {output_name}.safetensors")
        if resume_path:
            print(f"   (Í∏∞Ï°¥: {resume_path.name} ‚Üí ÏÉàÎ°úÏö¥: {output_name}.safetensors)")
        sys.exit(0)


if __name__ == "__main__":
    main()