#!/usr/bin/env python3
"""
SDXL LoRA Îã®Ïùº ÌïôÏäµ Ïä§ÌÅ¨Î¶ΩÌä∏ (Í≥†Í∏â ÏÇ¨Ïö©ÏûêÏö©)
- ÌäπÏ†ï Ìè¥ÎçîÎßå ÏÑ†ÌÉù ÌïôÏäµ
- ÏÑ∏Î∞ÄÌïú ÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ï†ï Í∞ÄÎä•
- Config Ïò§Î≤ÑÎùºÏù¥Îìú
"""

import os
import sys
import logging
import re
import toml
import subprocess
import argparse
from pathlib import Path
from run_train_auto import TrainingConfig, LoRATrainer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # ÌÑ∞ÎØ∏ÎÑêÎ°ú
        logging.FileHandler("/app/sdxl_train_captioner/logs/train_single_debug.log")  # ÌååÏùºÎ°ú
    ]
)

def get_vram_size(gpu_id=0):
    """NVIDIA GPU VRAM ÌÅ¨Í∏∞ Í∞êÏßÄ (GB)"""
    try:
        cmd = [
            "nvidia-smi",
            "--query-gpu=memory.total",
            "--format=csv,noheader,nounits",
            "-i", str(gpu_id)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        vram_mb = int(result.stdout.strip())
        return vram_mb // 1024
    except:
        return 24  # Í∏∞Î≥∏Í∞í


def count_images(folder_path):
    """Ìè¥Îçî ÎÇ¥ Ïù¥ÎØ∏ÏßÄ Í∞úÏàò ÏÑ∏Í∏∞ (1Îã®Í≥Ñ ÌïòÏúÑ Ìè¥ÎçîÍπåÏßÄ Ìè¨Ìï®)"""
    extensions = {'.jpg', '.jpeg', '.png', '.webp', '.bmp'}
    count = 0

    folder_path = Path(folder_path)

    # ÏÉÅÏúÑ Ìè¥Îçî ÏïàÏùò ÏßÅÏ†ë Ïù¥ÎØ∏ÏßÄ
    for file in folder_path.iterdir():
        if file.is_file() and file.suffix.lower() in extensions:
            count += 1

    # Î∞îÎ°ú ÏïÑÎûò 1Îã®Í≥Ñ ÌïòÏúÑ Ìè¥Îçî Ïù¥ÎØ∏ÏßÄ
    for subfolder in folder_path.iterdir():
        if subfolder.is_dir():
            for file in subfolder.iterdir():
                if file.is_file() and file.suffix.lower() in extensions:
                    count += 1

    return count


def calculate_auto_params(image_count, vram_size, batch_size=1):
    """Ïù¥ÎØ∏ÏßÄ Ïàò Í∏∞Î∞ò ÏûêÎèô ÌååÎùºÎØ∏ÌÑ∞ Í≥ÑÏÇ∞"""
    target_steps = 1800 if vram_size >= 20 else 1500

    # Repeats Í≥ÑÏÇ∞
    if image_count < 20:
        repeats = max(80, min(200, target_steps // (image_count * 10)))
    elif image_count < 50:
        repeats = max(30, min(80, target_steps // (image_count * 10)))
    elif image_count < 100:
        repeats = max(15, min(30, target_steps // (image_count * 10)))
    else:
        repeats = max(5, min(20, target_steps // (image_count * 10)))

    # Epochs Í≥ÑÏÇ∞
    images_per_epoch = image_count * repeats
    steps_per_epoch = images_per_epoch // batch_size
    epochs = max(1, round(target_steps / steps_per_epoch))
    epochs = min(max(epochs, 5), 30)

    return {
        'repeats': repeats,
        'epochs': epochs,
        'steps_per_epoch': steps_per_epoch,
        'total_steps': epochs * steps_per_epoch
    }


def main():
    parser = argparse.ArgumentParser(
        description="SDXL LoRA Îã®Ïùº ÌïôÏäµ (Í≥†Í∏â ÏÑ§Ï†ï)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""(ÏÉùÎûµ: ÏÇ¨Ïö© ÏòàÏãú ÎèôÏùº)"""
    )

    # -- Ïù∏Ïûê Ï†ïÏùò (ÏÉùÎûµ: Í∏∞Ï°¥Í≥º ÎèôÏùº) --
    # parser.add_argument(...) Î∏îÎ°ùÎì§

    args = parser.parse_args()

    # ==========================================
    # 1. Í∏∞Î≥∏ Í≤ÄÏ¶ù
    # ==========================================
    folder_path = Path(args.folder)
    if not folder_path.exists() or not folder_path.is_dir():
        print(f"‚ùå Ìè¥ÎçîÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {folder_path}")
        sys.exit(1)

    image_count = count_images(folder_path)
    if image_count == 0:
        print(f"‚ùå Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§: {folder_path}")
        sys.exit(1)

    # ==========================================
    # 2. Resume ÌååÏùº Í≤ÄÏ¶ù (ÏµúÏö∞ÏÑ† Ï≤òÎ¶¨)
    # ==========================================
    resume_path = None
    if args.resume:
        resume_path = Path(args.resume).resolve()
        if not resume_path.exists():
            print(f"‚ùå Resume ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {resume_path}")
            sys.exit(1)
        if resume_path.suffix.lower() != '.safetensors':
            print(f"‚ö†Ô∏è  Í≤ΩÍ≥†: Resume ÌååÏùºÏù¥ .safetensorsÍ∞Ä ÏïÑÎãôÎãàÎã§: {resume_path}")

    # ==========================================
    # 3. VRAM Î∞è Config ÏÑ§Ï†ï
    # ==========================================
    vram_size = get_vram_size(args.gpu)

    if vram_size >= 20:
        precision = "bf16"
        config_file = args.config if args.config else "config-24g.toml"
    else:
        precision = "fp16"
        config_file = args.config if args.config else "config-16g.toml"
        print(f"‚ö†Ô∏è VRAM {vram_size}GB < 20GB, fp16 Î™®ÎìúÎ°ú Ï†ÑÌôò")

    if not os.path.exists(config_file):
        print(f"‚ùå Config ÌååÏùº ÏóÜÏùå: {config_file}")
        sys.exit(1)

    with open(config_file, 'r', encoding='utf-8') as f:
        config = toml.load(f)

    batch_size = args.batch_size or config['training'].get('batch_size', 1)

    # ==========================================
    # 4. Ï∂úÎ†• Ïù¥Î¶Ñ Í≤∞Ï†ï
    # ==========================================
    if args.output:
        output_name = args.output
    else:
        folder_name = folder_path.name
        parts = folder_name.split('_', 1)
        if len(parts) == 2 and parts[0].isdigit():
            base_name = parts[1]
        else:
            base_name = folder_name
        # ÌÅ¥ÎûòÏä§ Ï†ëÎØ∏ÏÇ¨ Ï†úÍ±∞
        base_name = re.sub(r'_[a-zA-Z0-9]+$', '', base_name)
        output_name = base_name

    # ==========================================
    # 5. ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞ Í≤∞Ï†ï
    # ==========================================
    if args.no_auto:
        if not args.epochs or not args.repeats:
            print("‚ùå --no-auto ÏÇ¨Ïö© Ïãú --epochsÏôÄ --repeats ÌïÑÏàòÏûÖÎãàÎã§")
            sys.exit(1)
        epochs = args.epochs
        repeats = args.repeats
    else:
        auto_params = calculate_auto_params(image_count, vram_size, batch_size)
        epochs = args.epochs or auto_params['epochs']
        repeats = args.repeats or auto_params['repeats']

    steps_per_epoch = (image_count * repeats) // batch_size
    total_steps = epochs * steps_per_epoch

    # ==========================================
    # 6. ÌïôÏäµ Ï†ïÎ≥¥ Ï∂úÎ†•
    # ==========================================
    print(f"\n{'=' * 70}")
    print(f"üéØ SDXL LoRA Training - Single Mode")
    print(f"{'=' * 70}")
    print(f"üìÅ Folder:           {folder_path}")
    print(f"üíæ Output:           {output_name}.safetensors")
    print(f"üìã Config:           {config_file}")
    print(f"üñ•Ô∏è  GPU:              {args.gpu} ({vram_size}GB VRAM)")
    print(f"‚ö° Precision:        {precision}")

    if resume_path:
        print(f"üîÑ Resume from:      {resume_path}")
        print(f"   (Ïù¥Ïñ¥ÌïôÏäµ Î™®Îìú - Í∏∞Ï°¥ Í∞ÄÏ§ëÏπòÏóêÏÑú Í≥ÑÏÜç)")

    print(f"{'-' * 70}")
    print(f"üìä Training Parameters")
    print(f"{'-' * 70}")
    print(f"  Images:            {image_count}")
    print(f"  Repeats:           {repeats}" + (" (manual)" if args.repeats else " (auto)"))
    print(f"  Epochs:            {epochs}" + (" (manual)" if args.epochs else " (auto)"))
    print(f"  Batch size:        {batch_size}" + (" (override)" if args.batch_size else ""))
    print(f"  Images/epoch:      {image_count * repeats}")
    print(f"  Steps/epoch:       {steps_per_epoch}")
    print(f"  Total steps:       {total_steps}")
    print(f"{'=' * 70}\n")

    # ==========================================
    # 7. ÏÇ¨Ïö©Ïûê ÌôïÏù∏
    # ==========================================
    try:
        response = input("ÌïôÏäµÏùÑ ÏãúÏûëÌïòÏãúÍ≤†ÏäµÎãàÍπå? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("‚ùå ÌïôÏäµ Ï∑®ÏÜåÎê®")
            sys.exit(0)
    except KeyboardInterrupt:
        print("\n‚ùå ÌïôÏäµ Ï∑®ÏÜåÎê®")
        sys.exit(0)

    # ==========================================
    # 8. TrainingConfig ÏÉùÏÑ±
    # ==========================================
    try:
        training_config = TrainingConfig(
            config_file=config_file,
            gpu_id=args.gpu,
            force_repeats=args.repeats
        )

        # Resume ÏÑ§Ï†ï Ï∂îÍ∞Ä
        if resume_path:
            training_config.resume = str(resume_path)

        # Ï∂îÍ∞Ä Ïò§Î≤ÑÎùºÏù¥Îìú ÏÑ§Ï†ï
        if args.batch_size:
            training_config.batch_size = args.batch_size
        if args.lr:
            training_config.learning_rate = args.lr
        if args.dim:
            training_config.network_dim = args.dim
        if args.alpha:
            training_config.network_alpha = args.alpha
        if args.resolution:
            training_config.resolution = args.resolution

    except Exception as e:
        print(f"‚ùå TrainingConfig ÏÉùÏÑ± Ïã§Ìå®: {e}")
        sys.exit(1)

    # ==========================================
    # 9. Trainer ÏÉùÏÑ± Î∞è ÌïôÏäµ Ïã§Ìñâ
    # ==========================================
    trainer = LoRATrainer(training_config)

    # folder_info ÏÉùÏÑ±
    folder_name = folder_path.name
    parts = folder_name.split('_', 1)
    if len(parts) == 2 and parts[0].isdigit():
        order = int(parts[0])
        name = parts[1]
    else:
        order = 0
        name = folder_name

    folder_info = {
        'order': order,
        'name': name,
        'path': str(folder_path),
        'folder': folder_name,
        'category': folder_path.parent.name,
        'output_name': output_name,  # Ï∂úÎ†• Ïù¥Î¶Ñ Ï†ÑÎã¨
        'epochs': epochs,
        'repeats': repeats
    }

    # ÌïôÏäµ Ïã§Ìñâ
    print("\nüöÄ ÌïôÏäµ ÏãúÏûë...\n")
    success = trainer.train_single_lora(folder_info)

    if not success:
        print("\n‚ùå ÌïôÏäµ Ïã§Ìå®")
        sys.exit(1)
    else:
        print(f"\n‚úÖ ÌïôÏäµ ÏôÑÎ£å: {output_name}.safetensors")
        if resume_path:
            print(f"   (Í∏∞Ï°¥: {resume_path.name} ‚Üí ÏÉàÎ°úÏö¥: {output_name}.safetensors)")
        sys.exit(0)


if __name__ == "__main__":
    main()